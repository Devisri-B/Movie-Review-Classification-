{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fcb79f9",
   "metadata": {},
   "source": [
    "### Tasks:\n",
    "- pre-process features in different ways, as in the previous task:  \n",
    "    - lower case, stemmer (porter, ours)\n",
    "    - replace (NER)\n",
    "    - compute TF-IDF / w2v (1, 2, .. n-grams)\n",
    "    - filter most important terms/reduce vector size\n",
    "- compare precision, recall accuracy of classifiers for vector sizes\n",
    "- modify parameters of classifiers\n",
    "- fold cross validation (using different training/testing) sets\n",
    "- document and report in a markup cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b51a6c",
   "metadata": {},
   "source": [
    "###### Project I have done in few lines:\n",
    "1. In Preprocessing:\n",
    "    - For stemmer I used both porter and ours. I found ours is good so using only it. \n",
    "    - To replace NER and do lemmatization I used spacy and NLTK. I found spacy is good.\n",
    "    - I did either stemming or lemmatization separately, to find how this preprocessing effects vector size.\n",
    "    - I computed TF-IDF using TfidfVectorizer.\n",
    "    - I filterd most important words by removing stopwords,punctuations and using min_df parameter in TfidfVectorizer.\n",
    "    - I reduced vector size using latent sematic analysis.\n",
    "2. I compared precision,recall,accuracy,F1 score of both positive and negative vectors for all 4 classifiers\n",
    "3. I modified parameters of classifiers and did hyperparameter optimization using RandomizedSearchCV and GridSearchCV to find the optimal set of hyperparameters for a given model.\n",
    "4. I ran data split based on train_test_split multiple times to see the performance each time.  \n",
    "5. I did Stratified K-Fold cross validation to have different training and test sets and evaluated performance of all 4 classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "25009557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tree import Tree\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6acb7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our stemmer \n",
    "def myStemmerDic(tok, pos, dic):\n",
    "    stem = tok.lower()\n",
    "\n",
    "    # Check in the exception dictionary first\n",
    "    if stem in dic and pos in dic[stem]:\n",
    "        stem = dic[stem][pos]\n",
    "    \n",
    "    elif(pos == \"VBG\"):                      # Verb forms\n",
    "        stem = re.sub(r'ing$', '', stem)\n",
    "    elif(pos == \"VBD\"):\n",
    "        stem = re.sub(r'ed$', '', stem)\n",
    "    elif(pos == \"VBN\"):\n",
    "        stem = re.sub(r'ed$', '', stem)\n",
    "    elif(pos == \"VBZ\"):\n",
    "        stem = re.sub(r'(es|s)$', '', stem)\n",
    "        \n",
    "    elif(pos == \"NNS\"):                      # Noun forms\n",
    "        stem = re.sub(r'(s|ies)$', '', stem)\n",
    "        \n",
    "    elif(pos == \"JJR\"):                      # Adjective forms\n",
    "        stem = re.sub(r'(er)$', '', stem)               \n",
    "        \n",
    "    elif(pos == \"JJS\"):\n",
    "        stem = re.sub(r'(est)$', '', stem)\n",
    "        \n",
    "    elif(pos == \"RBR\"):                      # Adverb forms\n",
    "        stem = re.sub(r'(er)$', '', stem)               \n",
    "        \n",
    "    elif(pos == \"RBS\"):\n",
    "        stem = re.sub(r'(est)$', '', stem)                \n",
    "        \n",
    "    elif(pos == \"FW\"):                       # Foreign words\n",
    "        stem = re.sub(r'(o)$', '', stem)     \n",
    "        \n",
    "        \n",
    "    return (tok, stem, pos)\n",
    "\n",
    "# exception dictionary\n",
    "dico = {\n",
    "    \"is\": {\"VBZ\": \"be\"},\n",
    "    \"was\": {\"VBD\": \"be\"},\n",
    "    \"were\": {\"VBD\": \"be\"},\n",
    "    \"been\": {\"VBN\": \"be\"},\n",
    "    \"'m\": {\"VBP\": \"am\"},\n",
    "    \"'ve\": {\"VBP\": \"have\"},\n",
    "    \"n't\": {\"RB\": \"not\"},\n",
    "    \"met\": {\"VBD\": \"meet\"},\n",
    "    \"said\": {\"VBD\": \"say\"},\n",
    "    \"'ll\": {\"MD\": \"be\"},        #modal\n",
    "    \"'s\":{\"POS\": \"be\"},         #Possessive ending\n",
    "    \"thought\": {\"VBD\": \"think\"},\n",
    "    \"has\": {\"VBZ\": \"have\"},\n",
    "    \"him\": {\"PRP\": \"he\"},\n",
    "    \"his\": {\"PRP$\": \"he\"},\n",
    "    \"children\": {\"NNS\": \"child\"},\n",
    "    \"mice\": {\"NNS\": \"mouse\"},\n",
    "    \"goes\" : {\"VBD\" : \"go\"},\n",
    "    \"died\": {\"VBD\": \"die\"},\n",
    "    \"had\": {\"VBD\": \"have\"},\n",
    "    \"feet\": {\"NNS\": \"foot\"},\n",
    "    \"teeth\": {\"NNS\": \"tooth\"},\n",
    "    \"geese\": {\"NNS\": \"goose\"},\n",
    "    \"capturing\": {\"VBG\": \"capture\"},\n",
    "    \"sacrificing\": {\"VBG\": \"sacrifice\"},\n",
    "    \"believing\": {\"VBG\": \"believe\"},\n",
    "    \"took\": {\"VBD\": \"take\"},\n",
    "    \"saw\": {\"VBD\": \"see\"},\n",
    "    \"gone\": {\"VBN\": \"go\"},\n",
    "    \"ate\": {\"VBD\": \"eat\"},\n",
    "    \"drank\": {\"VBD\": \"drink\"},\n",
    "    \"loved\": {\"VBN\": \"love\"},\n",
    "    \n",
    "    # Archaic forms and irregulars\n",
    "    \"thou\": {\"PRP\": \"you\"},\n",
    "    \"art\": {\"VBP\": \"are\"},\n",
    "    \"hath\": {\"VBZ\": \"have\"},\n",
    "    \"doth\": {\"VBZ\": \"do\"},\n",
    "    \"thy\": {\"PRP$\": \"your\"},\n",
    "    \"thee\": {\"PRP\": \"you\"},\n",
    "\n",
    "\n",
    "    # Irregular plurals\n",
    "    \"brethren\": {\"NNS\": \"brother\"},\n",
    "    \"oxen\": {\"NNS\": \"ox\"},\n",
    "    \"indices\": {\"NNS\": \"index\"},\n",
    "    \"appendices\": {\"NNS\": \"appendix\"}\n",
    "       \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a8b1a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming \n",
    "def stemming(sent):\n",
    "    \n",
    "    #stemming with our stemmer\n",
    "    gstem = [myStemmerDic(tok, tag, dico) for tok, tag in nltk.pos_tag(sent)]\n",
    "    stems_ours = [stem for tok, stem, tag in gstem]\n",
    "    \n",
    "    #stemming with porter stemmer \n",
    "    #porter   = PorterStemmer()\n",
    "    #stems_porter = [porter.stem(w) for w in sent]\n",
    "   \n",
    "    #return stems_porter\n",
    "    return stems_ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1938cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization using NLTK\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "def lemmatize_nltk(words):\n",
    "    lemmas =[]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word, tag in nltk.pos_tag(words):\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag is None:    # no supply of tag in case of None\n",
    "            lemma = lemmatizer.lemmatize(word) \n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wntag) \n",
    "        lemmas.append(lemma)\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0cdaa461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NER using NLTK\n",
    "\n",
    "def NER_NLTK(word):\n",
    "    \n",
    "    tokens = word_tokenize(word)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    chunked_ner = ne_chunk(tagged_tokens)            #apply ner chunking\n",
    "\n",
    "    processed_tokens = []\n",
    "\n",
    "    for chunk in chunked_ner:\n",
    "        if isinstance(chunk, Tree):\n",
    "            ner_label = chunk.label()  # Get the named entity label\n",
    "            processed_tokens.extend([ner_label] * len(chunk))  # Extend the list with the label, repeated for each token in the chunk\n",
    "        else:\n",
    "            word = chunk[0]   # If it's not an NE, just append the word,Chunk format is (word, POS_tag)\n",
    "            processed_tokens.append(word)\n",
    "    return processed_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3fd9ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy for NER and lemmatization\n",
    "\n",
    "\n",
    "def spacy_ner(words,flag=0):\n",
    "    # Load the spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    docs = ' '.join([items for items in words])\n",
    "\n",
    "    # Process the text (join your tokens into a single string)\n",
    "    doc = nlp(docs)\n",
    "\n",
    "    processed_tokens = []\n",
    "    if flag == 0:\n",
    "        for token in doc:\n",
    "            if token.ent_type_:\n",
    "                processed_tokens.append(token.ent_type_)  #replace with NER \n",
    "            else:\n",
    "                processed_tokens.append(token.lemma_)  # replace with lemma for non-entity tokens\n",
    "    if flag == 1:\n",
    "        for token in doc:\n",
    "            if token.ent_type_:\n",
    "                processed_tokens.append(token.ent_type_)  #replace with NER \n",
    "            else:\n",
    "                processed_tokens.append(token.text)  # replace with text for non-entity tokens\n",
    "    return processed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9858b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter most important terms\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Delete function words|Remove stop words\n",
    "    filtered_tokens = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "\n",
    "def remove_punctuations(texts):\n",
    "    \n",
    "    text1 = ' '.join(texts)\n",
    "    tokens = word_tokenize(text1)\n",
    "    \n",
    "    # Filter out tokens that are entirely punctuation\n",
    "    cleaned_tokens = \" \".join([token for token in tokens if token not in string.punctuation])\n",
    "    \n",
    "    #removing custom punctuations not removed previously\n",
    "    cleaned_tokens = re.sub((\"[-`]\"),\" \",cleaned_tokens)\n",
    "    \n",
    "    token = word_tokenize(cleaned_tokens)\n",
    "    \n",
    "    #removing br \n",
    "    cleaned_texts = [text for text in token if text != \"br\"]\n",
    "\n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "229eabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize vectors by using latent sematic analysis\n",
    "\n",
    "def resizeVectors(M, size = 100):   \n",
    "    svd = TruncatedSVD(n_components = size)\n",
    "    return svd.fit_transform(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0ff9c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing \n",
    "\n",
    "def preprocessSent(sent):\n",
    "    \n",
    "    #using spacy for NER and lemmatization\n",
    "    data_lowercase = sent.lower()             #converting to lower case\n",
    "    \n",
    "    tokens = word_tokenize(data_lowercase)     \n",
    "    \n",
    "    filtered_data1 = remove_stopwords(tokens)     #removing stopwords\n",
    "    \n",
    "    ner = spacy_ner(filtered_data1,0)      #performing lemmatization and NER using spacy\n",
    "    \n",
    "    #ner = spacy_ner(filtered_data1,1)      #performing just NER using spacy\n",
    "\n",
    "    #stemmed_data = stemming(ner)       #stemming \n",
    "    \n",
    "    #filtered_data2 = remove_punctuations(stemmed_data)    #removing punctuations\n",
    "    \n",
    "    filtered_data2 = remove_punctuations(ner)    #removing punctuations\n",
    "    \n",
    "    processed_text = ' '.join(filtered_data2)\n",
    "    \n",
    "    return processed_text\n",
    "    \"\"\" \n",
    "    #using NLTK for NER and lemmatization\n",
    "    replace_NER = NER_NLTK(sent)       #performing NER using NLTK\n",
    "    \n",
    "    filtered_docs = ' '.join([items for items in replace_NER])\n",
    "    \n",
    "    #lemmatized_data = lemmatize_nltk(replace_NER)    #performing lemmatization using NLTK\n",
    "    \n",
    "    #filtered_docs = ' '.join([items for items in lemmatized_data])\n",
    "    \n",
    "    data_lowercase = filtered_docs.lower()\n",
    "    \n",
    "    tokens = word_tokenize(data_lowercase)\n",
    "    \n",
    "    filtered_data1 = remove_stopwords(tokens)     #removing stopwords\n",
    "    \n",
    "    stemmed_data = stemming(filtered_data1)      #stemming\n",
    "    \n",
    "    filtered_data2 = remove_punctuations(stemmed_data)     #removing punctuations\n",
    "    \n",
    "    #filtered_data2 = remove_punctuations(filtered_data1)     #removing punctuations\n",
    "     \n",
    "    processed_text = ' '.join(filtered_data2)\n",
    "    \n",
    "    return processed_text\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3ca1ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Pos:100 #words in Docs:91702\n",
      "#Neg:100 #words in Docs:74529\n",
      "#Sum:200 #words in Docs:166231\n",
      "Length of Tfidf vectors: 3967\n",
      "Docs vector after tfidf: #Pos:(100, 3967) #Neg:(100, 3967) \n",
      "Docs vector after reducing size & tfidf: #Pos:(100, 100) #Neg:(100, 100) \n"
     ]
    }
   ],
   "source": [
    "def ReadSourceTok(dic, n=100,  tag = False, verbose = 0) :\n",
    "    D = {}\n",
    "    i = 0\n",
    "\n",
    "    # Read sorted file names\n",
    "    for f in sorted(Path(dic).iterdir()):\n",
    "        if(verbose == 1): print(f.resolve())\n",
    "        if (i == n): break\n",
    "        i += 1\n",
    "        \n",
    "        with f.open('r', encoding='utf-8') as fhin: data = fhin.read()\n",
    "            \n",
    "        # get the file basename as index for document\n",
    "        b = os.path.basename(f).split(\".\")[0]\n",
    "        \n",
    "        # document is a string of tokens\n",
    "        D[b] = preprocessSent(data)\n",
    "    return D\n",
    "\n",
    "# number of docs to read\n",
    "nDocs = 100\n",
    "\n",
    "#initialize \n",
    "D1 = N1 = P1 = {}\n",
    "\n",
    "# Read positive documents \n",
    "P1 = ReadSourceTok(\"/data/critt/shared/resources/aclImdb/test/pos/\", n=nDocs, tag=False)\n",
    "\n",
    "# Read negative documents \n",
    "N1 = ReadSourceTok(\"/data/critt/shared/resources/aclImdb/test/neg/\", n=nDocs, tag=False)\n",
    "\n",
    "# join the Pos and the Neg corpus\n",
    "D1 = {**P1,**N1}\n",
    "\n",
    "print(f\"#Pos:{len(P1)} #words in Docs:{len([w for d in P1.keys() for s in P1[d] for w in s])}\")\n",
    "print(f\"#Neg:{len(N1)} #words in Docs:{len([w for d in N1.keys() for s in N1[d] for w in s])}\")\n",
    "print(f\"#Sum:{len(D1)} #words in Docs:{len([w for d in D1.keys() for s in D1[d] for w in s])}\")\n",
    "\n",
    "\n",
    "# tfidf vector 1 - 3 grams\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 3),min_df=2)  \n",
    "\n",
    "# Learn vocabulary and idf from P1 and N1 documents\n",
    "tfidf.fit(D1.values())     #produces length of vector\n",
    "\n",
    "print(f\"Length of Tfidf vectors: {len(tfidf.get_feature_names())}\")\n",
    "\n",
    " \n",
    "# Transform P1 documents to document-term matrix.\n",
    "Pos1 = tfidf.transform(P1.values())\n",
    "\n",
    "# Transform N1 documents to document-term matrix.\n",
    "Neg1 = tfidf.transform(N1.values())\n",
    "\n",
    "# Transform Pos1 and Neg1 documents to document-term matrix.\n",
    "Ptr1 = Pos1.toarray()\n",
    "Ntr1 = Neg1.toarray()\n",
    "\n",
    "print(f\"Docs vector after tfidf: #Pos:{Ptr1.shape} #Neg:{Ntr1.shape} \")\n",
    "\n",
    "# Applying Latent semantic analysis\n",
    "lsa_Pos = resizeVectors(Pos1,100)\n",
    "lsa_Neg = resizeVectors(Neg1,100)\n",
    "\n",
    "print(f\"Docs vector after reducing size & tfidf: #Pos:{lsa_Pos.shape} #Neg:{lsa_Neg.shape} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3ac8f7",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- Using TfidfVectorizer rather than CountVectorizer because TfidfVectorizer gives importance not only  to raw frequency of terms but also their relevance to the document.\n",
    "- min_df=2 in TfidfVectorizer includes terms that appear in at least 2 documents thus considering most important terms.\n",
    "- Used latent sematic analysis to reduce vector size because PCA doesnot work on sparse data and giving Valueerro\n",
    "- Vectors after tfidf are #Pos:(100, 4135) #Neg:(100, 4135). When vector size reduced using latent sematic analysis number of vectors is #Pos:(100, 100) #Neg:(100, 100) \n",
    "- the output from fit_transform will be (n_samples, n_components), where n_samples is the number of samples (or documents) and n_components is the number of dimensions.\n",
    "   - here, #Pos:(100, 100) #Neg:(100, 100) means 100 samples with 100 dimensions\n",
    "\n",
    "###### For 100 documents,\n",
    "\n",
    "- If preprocessing and vector size reduction is not done.\n",
    "    - #Pos:100 #words in Docs:151077\n",
    "    - #Neg:100 #words in Docs:125107\n",
    "    - #Sum:200 #words in Docs:276184\n",
    "    - Length of Tfidf vectors: 78851\n",
    "    \n",
    "    \n",
    "- In preprocessing did spacy(NER,lemmatization),filter most important terms and reducing vector size\n",
    "    - #Pos:100 #words in Docs:91702\n",
    "    - #Neg:100 #words in Docs:74529\n",
    "    - #Sum:200 #words in Docs:166231\n",
    "    - Length of Tfidf vectors:3967    \n",
    "    \n",
    "    \n",
    "- In preprocessing did spacy(NER),stemming(ours),filter most important terms and reducing vector size\n",
    "    - #Pos:100 #words in Docs:91115\n",
    "    - #Neg:100 #words in Docs:74090\n",
    "    - #Sum:200 #words in Docs:165205\n",
    "    - Length of Tfidf vectors:4136 \n",
    "    \n",
    "\n",
    "- In preprocessing did NLTK(NER,lemmatization),filter most important terms and reducing vector size\n",
    "    - #Pos:100 #words in Docs:92840\n",
    "    - #Neg:100 #words in Docs:75188\n",
    "    - #Sum:200 #words in Docs:168028\n",
    "    - Length of Tfidf vectors:3996\n",
    "  \n",
    "  \n",
    "- In preprocessing did NLTK(NER),stemming(ours),filter most important terms and reducing vector size\n",
    "    - #Pos:100 #words in Docs:92102\n",
    "    - #Neg:100 #words in Docs:74515\n",
    "    - #Sum:200 #words in Docs:166617\n",
    "    - Length of Tfidf vectors:4107\n",
    "   \n",
    "   \n",
    " - Out of all (In preprocessing did spacy(NER,lemmatization),filter most important terms and reducing vector size) is giving small number of tfidf vectors 3967.\n",
    " - Lemmatization(either by NLTK-3996 or Spacy-3967) gave less number of tfidf vectors than stemming(eithrt by NLTK-4107 or Spacy-4136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b9c5280a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.198724</td>\n",
       "      <td>-0.174723</td>\n",
       "      <td>-0.046560</td>\n",
       "      <td>-0.123015</td>\n",
       "      <td>-0.049170</td>\n",
       "      <td>0.118192</td>\n",
       "      <td>-0.034534</td>\n",
       "      <td>-0.032467</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>0.078233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006468</td>\n",
       "      <td>-0.026142</td>\n",
       "      <td>-0.000777</td>\n",
       "      <td>-0.003682</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>0.014366</td>\n",
       "      <td>-0.002095</td>\n",
       "      <td>0.003779</td>\n",
       "      <td>1.734723e-18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.244216</td>\n",
       "      <td>-0.147893</td>\n",
       "      <td>-0.176419</td>\n",
       "      <td>0.065491</td>\n",
       "      <td>0.059865</td>\n",
       "      <td>-0.122240</td>\n",
       "      <td>0.242151</td>\n",
       "      <td>-0.057371</td>\n",
       "      <td>-0.111875</td>\n",
       "      <td>0.075092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037450</td>\n",
       "      <td>-0.012131</td>\n",
       "      <td>-0.031259</td>\n",
       "      <td>-0.016767</td>\n",
       "      <td>-0.011999</td>\n",
       "      <td>0.007542</td>\n",
       "      <td>-0.014171</td>\n",
       "      <td>-0.000992</td>\n",
       "      <td>5.854692e-17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.206636</td>\n",
       "      <td>-0.085652</td>\n",
       "      <td>-0.130101</td>\n",
       "      <td>0.124288</td>\n",
       "      <td>0.032111</td>\n",
       "      <td>-0.125754</td>\n",
       "      <td>0.363288</td>\n",
       "      <td>0.023746</td>\n",
       "      <td>-0.110371</td>\n",
       "      <td>-0.017236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009186</td>\n",
       "      <td>0.029322</td>\n",
       "      <td>-0.213395</td>\n",
       "      <td>0.068549</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>-0.005848</td>\n",
       "      <td>-0.003823</td>\n",
       "      <td>-0.013296</td>\n",
       "      <td>1.027824e-16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.199278</td>\n",
       "      <td>-0.175578</td>\n",
       "      <td>-0.139687</td>\n",
       "      <td>-0.111703</td>\n",
       "      <td>-0.020737</td>\n",
       "      <td>0.031985</td>\n",
       "      <td>0.149586</td>\n",
       "      <td>-0.162077</td>\n",
       "      <td>-0.089015</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012831</td>\n",
       "      <td>0.031061</td>\n",
       "      <td>0.010201</td>\n",
       "      <td>0.025158</td>\n",
       "      <td>-0.004957</td>\n",
       "      <td>-0.004915</td>\n",
       "      <td>0.004010</td>\n",
       "      <td>-0.024305</td>\n",
       "      <td>-2.679335e-17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.267070</td>\n",
       "      <td>-0.143570</td>\n",
       "      <td>-0.109298</td>\n",
       "      <td>0.171315</td>\n",
       "      <td>0.074242</td>\n",
       "      <td>-0.089133</td>\n",
       "      <td>0.469048</td>\n",
       "      <td>-0.009555</td>\n",
       "      <td>-0.098383</td>\n",
       "      <td>0.012951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.053236</td>\n",
       "      <td>-0.152597</td>\n",
       "      <td>0.019948</td>\n",
       "      <td>-0.001764</td>\n",
       "      <td>-0.010894</td>\n",
       "      <td>0.006407</td>\n",
       "      <td>-0.016624</td>\n",
       "      <td>-1.184762e-16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.165220</td>\n",
       "      <td>0.025094</td>\n",
       "      <td>-0.168507</td>\n",
       "      <td>0.216939</td>\n",
       "      <td>-0.176450</td>\n",
       "      <td>-0.035032</td>\n",
       "      <td>-0.030426</td>\n",
       "      <td>-0.100123</td>\n",
       "      <td>0.259880</td>\n",
       "      <td>-0.231463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030119</td>\n",
       "      <td>0.084340</td>\n",
       "      <td>-0.171010</td>\n",
       "      <td>0.097927</td>\n",
       "      <td>0.020588</td>\n",
       "      <td>-0.186042</td>\n",
       "      <td>0.162486</td>\n",
       "      <td>-0.196609</td>\n",
       "      <td>-4.230572e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.212398</td>\n",
       "      <td>-0.093532</td>\n",
       "      <td>-0.188379</td>\n",
       "      <td>0.172079</td>\n",
       "      <td>-0.152760</td>\n",
       "      <td>0.012334</td>\n",
       "      <td>-0.107653</td>\n",
       "      <td>-0.113433</td>\n",
       "      <td>0.134218</td>\n",
       "      <td>-0.132996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013144</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>-0.022474</td>\n",
       "      <td>-0.082154</td>\n",
       "      <td>0.047798</td>\n",
       "      <td>-0.080510</td>\n",
       "      <td>0.029634</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>9.271991e-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.239396</td>\n",
       "      <td>-0.114906</td>\n",
       "      <td>0.043967</td>\n",
       "      <td>-0.169853</td>\n",
       "      <td>0.118497</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>-0.061852</td>\n",
       "      <td>0.100628</td>\n",
       "      <td>0.358114</td>\n",
       "      <td>0.102758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.062694</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.029284</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.064021</td>\n",
       "      <td>0.008165</td>\n",
       "      <td>0.089763</td>\n",
       "      <td>-6.240473e-04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.196572</td>\n",
       "      <td>-0.135702</td>\n",
       "      <td>-0.068218</td>\n",
       "      <td>-0.039095</td>\n",
       "      <td>-0.043260</td>\n",
       "      <td>0.058507</td>\n",
       "      <td>0.012039</td>\n",
       "      <td>-0.070316</td>\n",
       "      <td>-0.023330</td>\n",
       "      <td>-0.160980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035166</td>\n",
       "      <td>-0.026989</td>\n",
       "      <td>-0.035054</td>\n",
       "      <td>0.017161</td>\n",
       "      <td>0.010305</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.055054</td>\n",
       "      <td>0.006673</td>\n",
       "      <td>-3.242634e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.197688</td>\n",
       "      <td>-0.124207</td>\n",
       "      <td>-0.038111</td>\n",
       "      <td>-0.057201</td>\n",
       "      <td>-0.027119</td>\n",
       "      <td>-0.002534</td>\n",
       "      <td>-0.020482</td>\n",
       "      <td>0.043696</td>\n",
       "      <td>0.037025</td>\n",
       "      <td>-0.115731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036417</td>\n",
       "      <td>-0.030400</td>\n",
       "      <td>0.013925</td>\n",
       "      <td>0.024631</td>\n",
       "      <td>-0.012367</td>\n",
       "      <td>-0.076080</td>\n",
       "      <td>0.007997</td>\n",
       "      <td>-0.020594</td>\n",
       "      <td>-6.680335e-03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.198724 -0.174723 -0.046560 -0.123015 -0.049170  0.118192 -0.034534   \n",
       "1   0.244216 -0.147893 -0.176419  0.065491  0.059865 -0.122240  0.242151   \n",
       "2   0.206636 -0.085652 -0.130101  0.124288  0.032111 -0.125754  0.363288   \n",
       "3   0.199278 -0.175578 -0.139687 -0.111703 -0.020737  0.031985  0.149586   \n",
       "4   0.267070 -0.143570 -0.109298  0.171315  0.074242 -0.089133  0.469048   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95  0.165220  0.025094 -0.168507  0.216939 -0.176450 -0.035032 -0.030426   \n",
       "96  0.212398 -0.093532 -0.188379  0.172079 -0.152760  0.012334 -0.107653   \n",
       "97  0.239396 -0.114906  0.043967 -0.169853  0.118497  0.000682 -0.061852   \n",
       "98  0.196572 -0.135702 -0.068218 -0.039095 -0.043260  0.058507  0.012039   \n",
       "99  0.197688 -0.124207 -0.038111 -0.057201 -0.027119 -0.002534 -0.020482   \n",
       "\n",
       "           7         8         9  ...        91        92        93        94  \\\n",
       "0  -0.032467  0.008728  0.078233  ...  0.006468 -0.026142 -0.000777 -0.003682   \n",
       "1  -0.057371 -0.111875  0.075092  ...  0.037450 -0.012131 -0.031259 -0.016767   \n",
       "2   0.023746 -0.110371 -0.017236  ...  0.009186  0.029322 -0.213395  0.068549   \n",
       "3  -0.162077 -0.089015  0.025001  ... -0.012831  0.031061  0.010201  0.025158   \n",
       "4  -0.009555 -0.098383  0.012951  ...  0.007843  0.053236 -0.152597  0.019948   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95 -0.100123  0.259880 -0.231463  ... -0.030119  0.084340 -0.171010  0.097927   \n",
       "96 -0.113433  0.134218 -0.132996  ...  0.013144  0.002013 -0.022474 -0.082154   \n",
       "97  0.100628  0.358114  0.102758  ...  0.032880  0.062694  0.001607  0.029284   \n",
       "98 -0.070316 -0.023330 -0.160980  ...  0.035166 -0.026989 -0.035054  0.017161   \n",
       "99  0.043696  0.037025 -0.115731  ... -0.036417 -0.030400  0.013925  0.024631   \n",
       "\n",
       "          95        96        97        98            99  Label  \n",
       "0   0.003469  0.014366 -0.002095  0.003779  1.734723e-18      1  \n",
       "1  -0.011999  0.007542 -0.014171 -0.000992  5.854692e-17      1  \n",
       "2   0.000651 -0.005848 -0.003823 -0.013296  1.027824e-16      1  \n",
       "3  -0.004957 -0.004915  0.004010 -0.024305 -2.679335e-17      1  \n",
       "4  -0.001764 -0.010894  0.006407 -0.016624 -1.184762e-16      1  \n",
       "..       ...       ...       ...       ...           ...    ...  \n",
       "95  0.020588 -0.186042  0.162486 -0.196609 -4.230572e-03      0  \n",
       "96  0.047798 -0.080510  0.029634  0.006300  9.271991e-04      0  \n",
       "97  0.007638  0.064021  0.008165  0.089763 -6.240473e-04      0  \n",
       "98  0.010305  0.004800  0.055054  0.006673 -3.242634e-03      0  \n",
       "99 -0.012367 -0.076080  0.007997 -0.020594 -6.680335e-03      0  \n",
       "\n",
       "[200 rows x 101 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainVecPos = pd.DataFrame(lsa_Pos)\n",
    "TrainVecPos[\"Label\"] = 1\n",
    "#TrainVecPos[\"Doc\"] = [d for d in P1]\n",
    "\n",
    "TrainVecNeg = pd.DataFrame(lsa_Neg)\n",
    "TrainVecNeg[\"Label\"] = 0\n",
    "#TrainVecNeg[\"Doc\"] = [d for d in N1]\n",
    "\n",
    "# merge dataset\n",
    "TrainVecSet3 = pd.concat([TrainVecPos, TrainVecNeg], axis=0)\n",
    "\n",
    "# Y is label: {1,0}\n",
    "Y = TrainVecSet3[['Label']]\n",
    "\n",
    "# X is everything without label\n",
    "X = TrainVecSet3.drop(['Label'], 1)\n",
    "\n",
    "\n",
    "TrainVecSet3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2c199d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:(150, 100) Labels Train: (150, 1)\tTest:(50, 100) Labels Test: (50, 1) \n"
     ]
    }
   ],
   "source": [
    "# extracting training and test set \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# extract train and test set first\n",
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size = .25)\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler = StandardScaler()   #standardizes features by removing the mean and scaling to unit variance.\n",
    "scaler.fit(trainX)         #computes the mean and standard deviation\n",
    "\n",
    "# scale both the training and test data\n",
    "# performing the standardization by centering and scaling based on the mean and standard deviation \n",
    "trainX_scaled = scaler.transform(trainX)\n",
    "testX_scaled = scaler.transform(testX)\n",
    "\n",
    "print(f\"Train:{trainX_scaled.shape} Labels Train: {trainY.shape}\\tTest:{testX_scaled.shape} Labels Test: {testY.shape} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8dd31a",
   "metadata": {},
   "source": [
    "If scaling is done on the entire dataset before splitting, you are not preventing data leakage from the test set \n",
    "into the training process. This means that information from the test set can influence the scaling parameters\n",
    "(mean and standard deviation)which is a mistake.The test set should be completely unseen data to simulate \n",
    "real-world performance accurately.Following is the wrong process:\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X)\n",
    "X_scale = scaler.transform(X)  \n",
    "trainX, testX, trainY, testY = train_test_split(X_scale, Y, test_size = .25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a038e61e",
   "metadata": {},
   "source": [
    "## Naive Bayesian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dde26b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a Gaussian Classifier\n",
    "NBmodel = GaussianNB(var_smoothing=1e-9)\n",
    "\n",
    "# Train the model using the training sets\n",
    "NBmodel.fit(trainX_scaled, trainY[\"Label\"])\n",
    "\n",
    "#Predict Output\n",
    "Y_Bayes = NBmodel.predict(testX_scaled)\n",
    "\n",
    "target_names = ['Neg', 'Pos']\n",
    "# Collect metrics\n",
    "report_nb = classification_report(testY[\"Label\"], Y_Bayes, target_names=target_names, output_dict=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946300ec",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "83b3104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate model with 200 decision trees\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,   # Number of trees\n",
    "    criterion='gini',   # Splitting criterion\n",
    "    max_depth=20,     # Maximum depth of the tree\n",
    "    max_features='sqrt', # Number of features to consider for the best split\n",
    "    bootstrap = False   #consider whole database or not\n",
    ")\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(trainX_scaled, trainY[\"Label\"]);\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "Y_rf = rf.predict(testX_scaled)\n",
    "\n",
    "target_names = ['Neg', 'Pos']\n",
    "report_rf = classification_report(testY[\"Label\"], Y_rf, target_names=target_names, output_dict=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe5b36",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "942f65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(         \n",
    "    kernel='linear',      # Specifies the kernel type to be used\n",
    "    gamma=\"scale\"       # Kernel coefficient\n",
    "  ) \n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(trainX_scaled, trainY[\"Label\"])\n",
    "\n",
    "#Predict the response for test dataset\n",
    "Y_clf = clf.predict(testX_scaled)\n",
    "\n",
    "target_names = ['Neg', 'Pos']\n",
    "report_clf = classification_report(testY[\"Label\"], Y_clf, target_names=target_names, output_dict = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a673020",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "38519d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# instatiate classifier\n",
    "mlp = MLPClassifier(\n",
    "    solver='adam',          # Solver for weight optimization\n",
    "    activation='tanh',       # Activation function\n",
    "    alpha=0.01,               # L2 penalty parameter\n",
    "    hidden_layer_sizes=(50,),  # Size of the hidden layer\n",
    "    random_state=1,\n",
    "    max_iter=1000,\n",
    "    learning_rate='adaptive',\n",
    "    early_stopping=True\n",
    ")\n",
    "mlp.fit(trainX_scaled, trainY[\"Label\"])\n",
    "\n",
    "pred_Y = mlp.predict(testX_scaled)\n",
    "\n",
    "target_names = ['Neg', 'Pos']\n",
    "report_mlp = classification_report(testY[\"Label\"], pred_Y, target_names=target_names,output_dict = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4e69c280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Neg)</th>\n",
       "      <th>Recall (Neg)</th>\n",
       "      <th>F1-Score (Neg)</th>\n",
       "      <th>Precision (Pos)</th>\n",
       "      <th>Recall (Pos)</th>\n",
       "      <th>F1-Score (Pos)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Classifier  Accuracy  Precision (Neg)  Recall (Neg)  F1-Score (Neg)  \\\n",
       "0    Naive Bayes      0.96         0.923077      1.000000        0.960000   \n",
       "1  Random Forest      0.86         0.814815      0.916667        0.862745   \n",
       "2            SVM      0.36         0.300000      0.250000        0.272727   \n",
       "3            MLP      0.50         0.476190      0.416667        0.444444   \n",
       "\n",
       "   Precision (Pos)  Recall (Pos)  F1-Score (Pos)  \n",
       "0         1.000000      0.923077        0.960000  \n",
       "1         0.913043      0.807692        0.857143  \n",
       "2         0.400000      0.461538        0.428571  \n",
       "3         0.517241      0.576923        0.545455  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Prepare data for DataFrame\n",
    "data = {\n",
    "    \"Classifier\": [\"Naive Bayes\", \"Random Forest\", \"SVM\", \"MLP\"],\n",
    "    \"Accuracy\": [report_nb[\"accuracy\"], report_rf[\"accuracy\"], report_clf[\"accuracy\"], report_mlp[\"accuracy\"]],\n",
    "    \"Precision (Neg)\": [report_nb[\"Neg\"][\"precision\"], report_rf[\"Neg\"][\"precision\"], report_clf[\"Neg\"][\"precision\"], report_mlp[\"Neg\"][\"precision\"]],\n",
    "    \"Recall (Neg)\": [report_nb[\"Neg\"][\"recall\"], report_rf[\"Neg\"][\"recall\"], report_clf[\"Neg\"][\"recall\"], report_mlp[\"Neg\"][\"recall\"]],\n",
    "    \"F1-Score (Neg)\": [report_nb[\"Neg\"][\"f1-score\"], report_rf[\"Neg\"][\"f1-score\"], report_clf[\"Neg\"][\"f1-score\"], report_mlp[\"Neg\"][\"f1-score\"]],\n",
    "    \"Precision (Pos)\": [report_nb[\"Pos\"][\"precision\"], report_rf[\"Pos\"][\"precision\"], report_clf[\"Pos\"][\"precision\"], report_mlp[\"Pos\"][\"precision\"]],\n",
    "    \"Recall (Pos)\": [report_nb[\"Pos\"][\"recall\"], report_rf[\"Pos\"][\"recall\"], report_clf[\"Pos\"][\"recall\"], report_mlp[\"Pos\"][\"recall\"]],\n",
    "    \"F1-Score (Pos)\": [report_nb[\"Pos\"][\"f1-score\"], report_rf[\"Pos\"][\"f1-score\"], report_clf[\"Pos\"][\"f1-score\"], report_mlp[\"Pos\"][\"f1-score\"]]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "comparison_df = pd.DataFrame(data)\n",
    "\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## results of train_test_split data ran multiple times\n",
    "\n",
    "Classifier\tAccuracy\tPrecision (Neg)\tRecall (Neg)\tF1-Score (Neg)\tPrecision (Pos)\tRecall (Pos)\tF1-Score (Pos)\n",
    "Naive Bayes\t0.98\t0.956522\t1\t0.977778\t1\t0.964286\t0.981818\n",
    "Random Forest\t0.8\t0.7\t0.954545\t0.807692\t0.95\t0.678571\t0.791667\n",
    "SVM\t0.28\t0.25\t0.318182\t0.28\t0.318182\t0.25\t0.28\n",
    "MLP\t0.46\t0.368421\t0.318182\t0.341463\t0.516129\t0.571429\t0.542373\n",
    "\n",
    "\n",
    "Naive Bayes\t0.98\t0.964286\t1\t0.981818\t1\t0.956522\t0.977778\n",
    "Random Forest\t0.94\t1\t0.888889\t0.941176\t0.884615\t1\t0.938776\n",
    "SVM\t0.22\t0.269231\t0.259259\t0.264151\t0.166667\t0.173913\t0.170213\n",
    "MLP\t0.54\t0.583333\t0.518519\t0.54902\t0.5\t0.565217\t0.530612\n",
    "\n",
    "\n",
    "Naive Bayes\t0.98\t0.962963\t1\t0.981132\t1\t0.958333\t0.978723\n",
    "Random Forest\t0.86\t0.827586\t0.923077\t0.872727\t0.904762\t0.791667\t0.844444\n",
    "SVM\t0.26\t0.28\t0.269231\t0.27451\t0.24\t0.25\t0.244898\n",
    "MLP\t0.56\t0.625\t0.384615\t0.47619\t0.529412\t0.75\t0.62069\n",
    "\n",
    "\n",
    "Naive Bayes\t0.86\t0.774194\t1\t0.872727\t1\t0.730769\t0.844444\n",
    "Random Forest\t0.84\t0.75\t1\t0.857143\t1\t0.692308\t0.818182\n",
    "SVM\t0.26\t0.259259\t0.291667\t0.27451\t0.26087\t0.230769\t0.244898\n",
    "MLP\t0.44\t0.375\t0.25\t0.3\t0.470588\t0.615385\t0.533333\n",
    "\n",
    "\n",
    "Naive Bayes\t1\t1\t1\t1\t1\t1\t1\n",
    "Random Forest\t0.9\t0.866667\t0.962963\t0.912281\t0.95\t0.826087\t0.883721\n",
    "SVM\t0.28\t0.304348\t0.259259\t0.28\t0.259259\t0.304348\t0.28\n",
    "MLP\t0.5\t0.555556\t0.37037\t0.444444\t0.46875\t0.652174\t0.545455\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0d2f75cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fold 1\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Classifier    |   Accuracy |   Precision (Neg) |   Recall (Neg) |   F1-Score (Neg) |   Precision (Pos) |   Recall (Pos) |   F1-Score (Pos) |\n",
       "|:--------------|-----------:|------------------:|---------------:|-----------------:|------------------:|---------------:|-----------------:|\n",
       "| Naive Bayes   |       1    |          1        |            1   |         1        |          1        |            1   |         1        |\n",
       "| Random Forest |       0.95 |          0.909091 |            1   |         0.952381 |          1        |            0.9 |         0.947368 |\n",
       "| SVM           |       0.2  |          0.25     |            0.3 |         0.272727 |          0.125    |            0.1 |         0.111111 |\n",
       "| MLP           |       0.65 |          0.714286 |            0.5 |         0.588235 |          0.615385 |            0.8 |         0.695652 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fold 2\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Classifier    |   Accuracy |   Precision (Neg) |   Recall (Neg) |   F1-Score (Neg) |   Precision (Pos) |   Recall (Pos) |   F1-Score (Pos) |\n",
       "|:--------------|-----------:|------------------:|---------------:|-----------------:|------------------:|---------------:|-----------------:|\n",
       "| Naive Bayes   |       1    |          1        |            1   |         1        |          1        |            1   |         1        |\n",
       "| Random Forest |       0.75 |          0.777778 |            0.7 |         0.736842 |          0.727273 |            0.8 |         0.761905 |\n",
       "| SVM           |       0.15 |          0.181818 |            0.2 |         0.190476 |          0.111111 |            0.1 |         0.105263 |\n",
       "| MLP           |       0.6  |          0.666667 |            0.4 |         0.5      |          0.571429 |            0.8 |         0.666667 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fold 3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Classifier    |   Accuracy |   Precision (Neg) |   Recall (Neg) |   F1-Score (Neg) |   Precision (Pos) |   Recall (Pos) |   F1-Score (Pos) |\n",
       "|:--------------|-----------:|------------------:|---------------:|-----------------:|------------------:|---------------:|-----------------:|\n",
       "| Naive Bayes   |       1    |          1        |            1   |         1        |          1        |            1   |         1        |\n",
       "| Random Forest |       0.95 |          0.909091 |            1   |         0.952381 |          1        |            0.9 |         0.947368 |\n",
       "| SVM           |       0.25 |          0.222222 |            0.2 |         0.210526 |          0.272727 |            0.3 |         0.285714 |\n",
       "| MLP           |       0.65 |          0.666667 |            0.6 |         0.631579 |          0.636364 |            0.7 |         0.666667 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fold 4\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Classifier    |   Accuracy |   Precision (Neg) |   Recall (Neg) |   F1-Score (Neg) |   Precision (Pos) |   Recall (Pos) |   F1-Score (Pos) |\n",
       "|:--------------|-----------:|------------------:|---------------:|-----------------:|------------------:|---------------:|-----------------:|\n",
       "| Naive Bayes   |       0.9  |          0.833333 |            1   |         0.909091 |          1        |            0.8 |         0.888889 |\n",
       "| Random Forest |       0.85 |          0.769231 |            1   |         0.869565 |          1        |            0.7 |         0.823529 |\n",
       "| SVM           |       0.25 |          0.142857 |            0.1 |         0.117647 |          0.307692 |            0.4 |         0.347826 |\n",
       "| MLP           |       0.25 |          0.222222 |            0.2 |         0.210526 |          0.272727 |            0.3 |         0.285714 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fold 5\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Classifier    |   Accuracy |   Precision (Neg) |   Recall (Neg) |   F1-Score (Neg) |   Precision (Pos) |   Recall (Pos) |   F1-Score (Pos) |\n",
       "|:--------------|-----------:|------------------:|---------------:|-----------------:|------------------:|---------------:|-----------------:|\n",
       "| Naive Bayes   |       0.95 |          0.909091 |            1   |         0.952381 |          1        |            0.9 |         0.947368 |\n",
       "| Random Forest |       0.85 |          0.769231 |            1   |         0.869565 |          1        |            0.7 |         0.823529 |\n",
       "| SVM           |       0.15 |          0.181818 |            0.2 |         0.190476 |          0.111111 |            0.1 |         0.105263 |\n",
       "| MLP           |       0.4  |          0.375    |            0.3 |         0.333333 |          0.416667 |            0.5 |         0.454545 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fold 6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Classifier    |   Accuracy |   Precision (Neg) |   Recall (Neg) |   F1-Score (Neg) |   Precision (Pos) |   Recall (Pos) |   F1-Score (Pos) |\n",
       "|:--------------|-----------:|------------------:|---------------:|-----------------:|------------------:|---------------:|-----------------:|\n",
       "| Naive Bayes   |        1   |          1        |            1   |         1        |               1   |            1   |         1        |\n",
       "| Random Forest |        0.9 |          0.833333 |            1   |         0.909091 |               1   |            0.8 |         0.888889 |\n",
       "| SVM           |        0.1 |          0.1      |            0.1 |         0.1      |               0.1 |            0.1 |         0.1      |\n",
       "| MLP           |        0.5 |          0.5      |            0.5 |         0.5      |               0.5 |            0.5 |         0.5      |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fold 7\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Classifier    |   Accuracy |   Precision (Neg) |   Recall (Neg) |   F1-Score (Neg) |   Precision (Pos) |   Recall (Pos) |   F1-Score (Pos) |\n",
       "|:--------------|-----------:|------------------:|---------------:|-----------------:|------------------:|---------------:|-----------------:|\n",
       "| Naive Bayes   |       1    |          1        |            1   |         1        |             1     |            1   |         1        |\n",
       "| Random Forest |       0.85 |          0.769231 |            1   |         0.869565 |             1     |            0.7 |         0.823529 |\n",
       "| SVM           |       0.15 |          0.230769 |            0.3 |         0.26087  |             0     |            0   |         0        |\n",
       "| MLP           |       0.6  |          0.583333 |            0.7 |         0.636364 |             0.625 |            0.5 |         0.555556 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fold 8\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Classifier    |   Accuracy |   Precision (Neg) |   Recall (Neg) |   F1-Score (Neg) |   Precision (Pos) |   Recall (Pos) |   F1-Score (Pos) |\n",
       "|:--------------|-----------:|------------------:|---------------:|-----------------:|------------------:|---------------:|-----------------:|\n",
       "| Naive Bayes   |       0.95 |          0.909091 |            1   |         0.952381 |          1        |            0.9 |         0.947368 |\n",
       "| Random Forest |       1    |          1        |            1   |         1        |          1        |            1   |         1        |\n",
       "| SVM           |       0.4  |          0.428571 |            0.6 |         0.5      |          0.333333 |            0.2 |         0.25     |\n",
       "| MLP           |       0.45 |          0.444444 |            0.4 |         0.421053 |          0.454545 |            0.5 |         0.47619  |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fold 9\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Classifier    |   Accuracy |   Precision (Neg) |   Recall (Neg) |   F1-Score (Neg) |   Precision (Pos) |   Recall (Pos) |   F1-Score (Pos) |\n",
       "|:--------------|-----------:|------------------:|---------------:|-----------------:|------------------:|---------------:|-----------------:|\n",
       "| Naive Bayes   |       1    |          1        |            1   |         1        |          1        |            1   |         1        |\n",
       "| Random Forest |       0.95 |          1        |            0.9 |         0.947368 |          0.909091 |            1   |         0.952381 |\n",
       "| SVM           |       0.25 |          0.272727 |            0.3 |         0.285714 |          0.222222 |            0.2 |         0.210526 |\n",
       "| MLP           |       0.5  |          0.5      |            0.4 |         0.444444 |          0.5      |            0.6 |         0.545455 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for fold 10\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Classifier    |   Accuracy |   Precision (Neg) |   Recall (Neg) |   F1-Score (Neg) |   Precision (Pos) |   Recall (Pos) |   F1-Score (Pos) |\n",
       "|:--------------|-----------:|------------------:|---------------:|-----------------:|------------------:|---------------:|-----------------:|\n",
       "| Naive Bayes   |       1    |          1        |            1   |         1        |          1        |            1   |         1        |\n",
       "| Random Forest |       1    |          1        |            1   |         1        |          1        |            1   |         1        |\n",
       "| SVM           |       0.3  |          0.25     |            0.2 |         0.222222 |          0.333333 |            0.4 |         0.363636 |\n",
       "| MLP           |       0.35 |          0.285714 |            0.2 |         0.235294 |          0.384615 |            0.5 |         0.434783 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#k-fold cross validation (using different training/testing) sets\n",
    "\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "\n",
    "\n",
    "# classifiers defined\n",
    "classifiers = {\n",
    "    'Naive Bayes': GaussianNB(var_smoothing=1e-9),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, criterion='gini', max_depth=20, max_features='sqrt', bootstrap=False),\n",
    "    'SVM' : svm.SVC(kernel = \"linear\"),\n",
    "    'MLP' : MLPClassifier(solver='adam',activation='tanh', alpha=0.01, hidden_layer_sizes=(50,), random_state=1,max_iter=1000,learning_rate='adaptive',early_stopping=True)\n",
    "\n",
    "}\n",
    "\n",
    "# metrics wanted to collect\n",
    "scoring_metrics = {\n",
    "    'precision_neg': make_scorer(precision_score, pos_label=0, zero_division=0),\n",
    "    'recall_neg': make_scorer(recall_score, pos_label=0, zero_division=0),\n",
    "    'f1_neg': make_scorer(f1_score, pos_label=0, zero_division=0),\n",
    "    'precision_pos': make_scorer(precision_score, pos_label=1, zero_division=0),\n",
    "    'recall_pos': make_scorer(recall_score, pos_label=1, zero_division=0),\n",
    "    'f1_pos': make_scorer(f1_score, pos_label=1, zero_division=0),\n",
    "    'accuracy': 'accuracy'\n",
    "}\n",
    "\n",
    "n_splits=10\n",
    "# Stratified K-Fold for maintaining label proportions\n",
    "cv = StratifiedKFold(n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Results dictionary\n",
    "results = {name: cross_validate(make_pipeline(StandardScaler(), clf), X, Y.values.ravel(), scoring=scoring_metrics, cv=cv, return_train_score=False) for name, clf in classifiers.items()}\n",
    "\n",
    "# For each fold and each classifier, print out a formatted table of results\n",
    "for fold_idx in range(n_splits):\n",
    "    print(f\"Results for fold {fold_idx + 1}\")\n",
    "    fold_results = []\n",
    "    \n",
    "    for clf_name, clf_scores in results.items():\n",
    "        fold_result = {\n",
    "            'Classifier': clf_name,\n",
    "            'Accuracy': clf_scores['test_accuracy'][fold_idx],\n",
    "            'Precision (Neg)': clf_scores['test_precision_neg'][fold_idx],\n",
    "            'Recall (Neg)': clf_scores['test_recall_neg'][fold_idx],\n",
    "            'F1-Score (Neg)': clf_scores['test_f1_neg'][fold_idx],\n",
    "            'Precision (Pos)': clf_scores['test_precision_pos'][fold_idx],\n",
    "            'Recall (Pos)': clf_scores['test_recall_pos'][fold_idx],\n",
    "            'F1-Score (Pos)': clf_scores['test_f1_pos'][fold_idx]\n",
    "        }\n",
    "        fold_results.append(fold_result)\n",
    "    \n",
    "    # Convert the fold results to DataFrame\n",
    "    fold_df = pd.DataFrame(fold_results)\n",
    "    \n",
    "    fold_markdown = fold_df.to_markdown(index=False)\n",
    "    \n",
    "    # Display the Markdown-formatted string \n",
    "    display(Markdown(fold_markdown))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92098b",
   "metadata": {},
   "source": [
    "- Out of all classifiers SVM is not performing well\n",
    "- some values are 0 for SVM because of the following error\n",
    "- UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use zero_division parameter to control this behavior.\n",
    "- It is because the denominator in their calculations (the number of positive predictions) is zero.\n",
    "- The zero_division parameter dictates what value to return when there is a division by zero during scoring. If zero_division=1, it means that if there are no positive predictions and therefore no true positives or false positives, the precision score is defined as 1. If zero_division=0, then the score is defined as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24874372",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization using RandomizedSearchCV or GridSearchCV is done to improve the performance of models.\n",
    "- Both aim to find the optimal set of hyperparameters for a given mode.\n",
    "- GridSearchCV performs an exhaustive search over a specified parameter grid.This method will systematically explore all possible combinations of the provided hyperparameters.\n",
    "- RandomizedSearchCV samples a fixed number of parameter settings from specified distributions.This method does not try every possible combination, but instead selects them randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "d483cd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Best parameters found:  {'random_state': 45, 'n_estimators': 200, 'max_features': 'sqrt', 'max_depth': None, 'criterion': 'gini', 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter optimization using RandomizedSearchCV for RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal\n",
    "\n",
    "# Define the parameter distribution\n",
    "param_distr = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'random_state' : [40,42,45],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Instantiate the random search model\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_distr,\n",
    "                                   n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search to the data\n",
    "random_search.fit(trainX_scaled, trainY.values.ravel())\n",
    "\n",
    "# random_search.best_params_ contains the best parameters found\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8e0901e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
      "Best parameters found:  {'C': 0.001, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter optimization using GridSearchCV for Support Vector Machine(SVM)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],\n",
    "    'gamma': ['scale', 0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'linear', 'poly']\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "sv = svm.SVC()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=sv, param_grid=param_grid, cv=5, n_jobs=4, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(trainX_scaled, trainY.values.ravel())\n",
    "\n",
    "# grid_search.best_params_ contains the best parameters found\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "cdbb0c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "Best parameters found:  {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'invscaling', 'random_state': None, 'solver': 'sgd'}\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter optimization using GridSearchCV for MLP\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'solver':['adam','lbfgs', 'sgd'],              # Solver for weight optimization\n",
    "    'alpha':[0.0001,1e-5] ,            \n",
    "    'random_state':[1,None] ,  \n",
    "    'learning_rate':['constant', 'invscaling', 'adaptive'],\n",
    "    'hidden_layer_sizes': [(50,), (100,), (200,)], # Size of the hidden layer\n",
    "    'activation': ['tanh', 'relu'],   # Activation function\n",
    "    'alpha': [0.0001, 0.001, 0.01]   # L2 penalty parameter\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(trainX_scaled, trainY.values.ravel())\n",
    "\n",
    "# grid_search.best_params_ contains the best parameters found\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4315bb4",
   "metadata": {},
   "source": [
    "## Understand how Preprocessing is happening based on 2 documents \n",
    "\n",
    "### Original text\n",
    "\n",
    "['i', 'went', 'and', 'saw', 'this', 'movie', 'last', 'night', 'after', 'being', 'coaxed', 'to', 'by', 'a', 'few', 'friends', 'of', 'mine', '.', 'i', \"'ll\", 'admit', 'that', 'i', 'was', 'reluctant', 'to', 'see', 'it', 'because', 'from', 'what', 'i', 'knew', 'of', 'ashton', 'kutcher', 'he', 'was', 'only', 'able', 'to', 'do', 'comedy', '.', 'i', 'was', 'wrong', '.', 'kutcher', 'played', 'the', 'character', 'of', 'jake', 'fischer', 'very', 'well', ',', 'and', 'kevin', 'costner', 'played', 'ben', 'randall', 'with', 'such', 'professionalism', '.', 'the', 'sign', 'of', 'a', 'good', 'movie', 'is', 'that', 'it', 'can', 'toy', 'with', 'our', 'emotions', '.', 'this', 'one', 'did', 'exactly', 'that', '.', 'the', 'entire', 'theater', '(', 'which', 'was', 'sold', 'out', ')', 'was', 'overcome', 'by', 'laughter', 'during', 'the', 'first', 'half', 'of', 'the', 'movie', ',', 'and', 'were', 'moved', 'to', 'tears', 'during', 'the', 'second', 'half', '.', 'while', 'exiting', 'the', 'theater', 'i', 'not', 'only', 'saw', 'many', 'women', 'in', 'tears', ',', 'but', 'many', 'full', 'grown', 'men', 'as', 'well', ',', 'trying', 'desperately', 'not', 'to', 'let', 'anyone', 'see', 'them', 'crying', '.', 'this', 'movie', 'was', 'great', ',', 'and', 'i', 'suggest', 'that', 'you', 'go', 'see', 'it', 'before', 'you', 'judge', '.']\n",
    "\n",
    "['actor', 'turned', 'director', 'bill', 'paxton', 'follows', 'up', 'his', 'promising', 'debut', ',', 'the', 'gothic-horror', '``', 'frailty', \"''\", ',', 'with', 'this', 'family', 'friendly', 'sports', 'drama', 'about', 'the', '1913', 'u.s.', 'open', 'where', 'a', 'young', 'american', 'caddy', 'rises', 'from', 'his', 'humble', 'background', 'to', 'play', 'against', 'his', 'bristish', 'idol', 'in', 'what', 'was', 'dubbed', 'as', '``', 'the', 'greatest', 'game', 'ever', 'played', '.', \"''\", 'i', \"'m\", 'no', 'fan', 'of', 'golf', ',', 'and', 'these', 'scrappy', 'underdog', 'sports', 'flicks', 'are', 'a', 'dime', 'a', 'dozen', '(', 'most', 'recently', 'done', 'to', 'grand', 'effect', 'with', '``', 'miracle', \"''\", 'and', '``', 'cinderella', 'man', \"''\", ')', ',', 'but', 'some', 'how', 'this', 'film', 'was', 'enthralling', 'all', 'the', 'same.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'film', 'starts', 'with', 'some', 'creative', 'opening', 'credits', '(', 'imagine', 'a', 'disneyfied', 'version', 'of', 'the', 'animated', 'opening', 'credits', 'of', 'hbo', \"'s\", '``', 'carnivale', \"''\", 'and', '``', 'rome', \"''\", ')', ',', 'but', 'lumbers', 'along', 'slowly', 'for', 'its', 'first', 'by-the-numbers', 'hour', '.', 'once', 'the', 'action', 'moves', 'to', 'the', 'u.s.', 'open', 'things', 'pick', 'up', 'very', 'well', '.', 'paxton', 'does', 'a', 'nice', 'job', 'and', 'shows', 'a', 'knack', 'for', 'effective', 'directorial', 'flourishes', '(', 'i', 'loved', 'the', 'rain-soaked', 'montage', 'of', 'the', 'action', 'on', 'day', 'two', 'of', 'the', 'open', ')', 'that', 'propel', 'the', 'plot', 'further', 'or', 'add', 'some', 'unexpected', 'psychological', 'depth', 'to', 'the', 'proceedings', '.', 'there', \"'s\", 'some', 'compelling', 'character', 'development', 'when', 'the', 'british', 'harry', 'vardon', 'is', 'haunted', 'by', 'images', 'of', 'the', 'aristocrats', 'in', 'black', 'suits', 'and', 'top', 'hats', 'who', 'destroyed', 'his', 'family', 'cottage', 'as', 'a', 'child', 'to', 'make', 'way', 'for', 'a', 'golf', 'course', '.', 'he', 'also', 'does', 'a', 'good', 'job', 'of', 'visually', 'depicting', 'what', 'goes', 'on', 'in', 'the', 'players', \"'\", 'heads', 'under', 'pressure', '.', 'golf', ',', 'a', 'painfully', 'boring', 'sport', ',', 'is', 'brought', 'vividly', 'alive', 'here', '.', 'credit', 'should', 'also', 'be', 'given', 'the', 'set', 'designers', 'and', 'costume', 'department', 'for', 'creating', 'an', 'engaging', 'period-piece', 'atmosphere', 'of', 'london', 'and', 'boston', 'at', 'the', 'beginning', 'of', 'the', 'twentieth', 'century.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'you', 'know', 'how', 'this', 'is', 'going', 'to', 'end', 'not', 'only', 'because', 'it', \"'s\", 'based', 'on', 'a', 'true', 'story', 'but', 'also', 'because', 'films', 'in', 'this', 'genre', 'follow', 'the', 'same', 'template', 'over', 'and', 'over', ',', 'but', 'paxton', 'puts', 'on', 'a', 'better', 'than', 'average', 'show', 'and', 'perhaps', 'indicates', 'more', 'talent', 'behind', 'the', 'camera', 'than', 'he', 'ever', 'had', 'in', 'front', 'of', 'it', '.', 'despite', 'the', 'formulaic', 'nature', ',', 'this', 'is', 'a', 'nice', 'and', 'easy', 'film', 'to', 'root', 'for', 'that', 'deserves', 'to', 'find', 'an', 'audience', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22a230e",
   "metadata": {},
   "source": [
    "#### After removing stop words\n",
    "\n",
    "['went', 'saw', 'movie', 'last', 'night', 'coaxed', 'friends', 'mine', '.', \"'ll\", 'admit', 'reluctant', 'see', 'knew', 'ashton', 'kutcher', 'able', 'comedy', '.', 'wrong', '.', 'kutcher', 'played', 'character', 'jake', 'fischer', 'well', ',', 'kevin', 'costner', 'played', 'ben', 'randall', 'professionalism', '.', 'sign', 'good', 'movie', 'toy', 'emotions', '.', 'one', 'exactly', '.', 'entire', 'theater', '(', 'sold', ')', 'overcome', 'laughter', 'first', 'half', 'movie', ',', 'moved', 'tears', 'second', 'half', '.', 'exiting', 'theater', 'saw', 'many', 'women', 'tears', ',', 'many', 'full', 'grown', 'men', 'well', ',', 'trying', 'desperately', 'let', 'anyone', 'see', 'crying', '.', 'movie', 'great', ',', 'suggest', 'go', 'see', 'judge', '.']\n",
    "\n",
    "['actor', 'turned', 'director', 'bill', 'paxton', 'follows', 'promising', 'debut', ',', 'gothic-horror', '``', 'frailty', \"''\", ',', 'family', 'friendly', 'sports', 'drama', '1913', 'u.s.', 'open', 'young', 'american', 'caddy', 'rises', 'humble', 'background', 'play', 'bristish', 'idol', 'dubbed', '``', 'greatest', 'game', 'ever', 'played', '.', \"''\", \"'m\", 'fan', 'golf', ',', 'scrappy', 'underdog', 'sports', 'flicks', 'dime', 'dozen', '(', 'recently', 'done', 'grand', 'effect', '``', 'miracle', \"''\", '``', 'cinderella', 'man', \"''\", ')', ',', 'film', 'enthralling', 'same.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'film', 'starts', 'creative', 'opening', 'credits', '(', 'imagine', 'disneyfied', 'version', 'animated', 'opening', 'credits', 'hbo', \"'s\", '``', 'carnivale', \"''\", '``', 'rome', \"''\", ')', ',', 'lumbers', 'along', 'slowly', 'first', 'by-the-numbers', 'hour', '.', 'action', 'moves', 'u.s.', 'open', 'things', 'pick', 'well', '.', 'paxton', 'nice', 'job', 'shows', 'knack', 'effective', 'directorial', 'flourishes', '(', 'loved', 'rain-soaked', 'montage', 'action', 'day', 'two', 'open', ')', 'propel', 'plot', 'add', 'unexpected', 'psychological', 'depth', 'proceedings', '.', \"'s\", 'compelling', 'character', 'development', 'british', 'harry', 'vardon', 'haunted', 'images', 'aristocrats', 'black', 'suits', 'top', 'hats', 'destroyed', 'family', 'cottage', 'child', 'make', 'way', 'golf', 'course', '.', 'also', 'good', 'job', 'visually', 'depicting', 'goes', 'players', \"'\", 'heads', 'pressure', '.', 'golf', ',', 'painfully', 'boring', 'sport', ',', 'brought', 'vividly', 'alive', '.', 'credit', 'also', 'given', 'set', 'designers', 'costume', 'department', 'creating', 'engaging', 'period-piece', 'atmosphere', 'london', 'boston', 'beginning', 'twentieth', 'century.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'know', 'going', 'end', \"'s\", 'based', 'true', 'story', 'also', 'films', 'genre', 'follow', 'template', ',', 'paxton', 'puts', 'better', 'average', 'show', 'perhaps', 'indicates', 'talent', 'behind', 'camera', 'ever', 'front', '.', 'despite', 'formulaic', 'nature', ',', 'nice', 'easy', 'film', 'root', 'deserves', 'find', 'audience', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97def93",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- Data should be converted to lower case before removing stopwords\n",
    "- for ex: letter i is a stopword but I is not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d41c94",
   "metadata": {},
   "source": [
    "#### stopwords removed from all positive documents are:\n",
    "\n",
    "['i', 'and', 'this', 'after', 'being', 'to', 'by', 'a', 'few', 'of', 'i', 'that', 'i', 'was', 'to', 'it', 'because', 'from', 'what', 'i', 'of', 'he', 'was', 'only', 'to', 'do', 'i', 'was', 'the', 'of', 'very', 'and', 'with', 'such', 'the', 'of', 'a', 'is', 'that', 'it', 'can', 'with', 'our', 'this', 'did', 'that', 'the', 'which', 'was', 'out', 'was', 'by', 'during', 'the', 'of', 'the', 'and', 'were', 'to', 'during', 'the', 'while', 'the', 'i', 'not', 'only', 'in', 'but', 'as', 'not', 'to', 'them', 'this', 'was', 'and', 'i', 'that', 'you', 'it', 'before', 'you']\n",
    "['up', 'his', 'the', 'with', 'this', 'about', 'the', 'where', 'a', 'from', 'his', 'to', 'against', 'his', 'in', 'what', 'was', 'as', 'the', 'i', 'no', 'of', 'and', 'these', 'are', 'a', 'a', 'most', 'to', 'with', 'and', 'but', 'some', 'how', 'this', 'was', 'all', 'the', 'the', 'with', 'some', 'a', 'of', 'the', 'of', 'and', 'but', 'for', 'its', 'once', 'the', 'to', 'the', 'up', 'very', 'does', 'a', 'and', 'a', 'for', 'i', 'the', 'of', 'the', 'on', 'of', 'the', 'that', 'the', 'further', 'or', 'some', 'to', 'the', 'there', 'some', 'when', 'the', 'is', 'by', 'of', 'the', 'in', 'and', 'who', 'his', 'as', 'a', 'to', 'for', 'a', 'he', 'does', 'a', 'of', 'what', 'on', 'in', 'the', 'under', 'a', 'is', 'here', 'should', 'be', 'the', 'and', 'for', 'an', 'of', 'and', 'at', 'the', 'of', 'the', 'you', 'how', 'this', 'is', 'to', 'not', 'only', 'because', 'it', 'on', 'a', 'but', 'because', 'in', 'this', 'the', 'same', 'over', 'and', 'over', 'but', 'on', 'a', 'than', 'and', 'more', 'the', 'than', 'he', 'had', 'in', 'of', 'it', 'the', 'this', 'is', 'a', 'and', 'to', 'for', 'that', 'to', 'an']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a069e",
   "metadata": {},
   "source": [
    "### after performing lemmatization and NER using Spacy\n",
    "\n",
    "['go', 'see', 'movie', 'TIME', 'TIME', 'coax', 'friend', '-PRON-', '.', 'will', 'admit', 'reluctant', 'see', 'know', 'ashton', 'kutcher', 'able', 'comedy', '.', 'wrong', '.', 'kutcher', 'play', 'character', 'jake', 'fischer', 'well', ',', 'kevin', 'costn', 'play', 'ben', 'randall', 'professionalism', '.', 'sign', 'good', 'movie', 'toy', 'emotion', '.', 'one', 'exactly', '.', 'entire', 'theater', '(', 'sell', ')', 'overcome', 'laughter', 'ORDINAL', 'CARDINAL', 'movie', ',', 'move', 'tear', 'ORDINAL', 'CARDINAL', '.', 'exit', 'theater', 'see', 'many', 'woman', 'tear', ',', 'many', 'full', 'grown', 'man', 'well', ',', 'try', 'desperately', 'let', 'anyone', 'see', 'cry', '.', 'movie', 'great', ',', 'suggest', 'go', 'see', 'judge', '.']\n",
    "\n",
    "['actor', 'turn', 'director', 'bill', 'paxton', 'follow', 'promise', 'debut', ',', 'gothic', '-', 'horror', '`', '`', 'frailty', \"''\", ',', 'family', 'friendly', 'sport', 'drama', 'DATE', 'u.s', '.', 'open', 'young', 'NORP', 'caddy', 'rise', 'humble', 'background', 'play', 'bristish', 'idol', 'dub', '`', '`', 'great', 'game', 'ever', 'play', '.', \"''\", \"'\", 'm', 'fan', 'golf', ',', 'scrappy', 'underdog', 'sport', 'flick', 'dime', 'dozen', '(', 'recently', 'do', 'grand', 'effect', '`', '`', 'miracle', \"''\", '`', '`', 'cinderella', 'man', \"''\", ')', ',', 'film', 'enthral', 'same', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'film', 'start', 'creative', 'opening', 'credit', '(', 'imagine', 'disneyfie', 'version', 'animate', 'opening', 'credit', 'hbo', \"'s\", '`', '`', 'carnivale', \"''\", '`', '`', 'rome', \"''\", ')', ',', 'lumber', 'along', 'slowly', 'ORDINAL', 'by', '-', 'the', '-', 'number', 'hour', '.', 'action', 'move', 'u.s', '.', 'open', 'thing', 'pick', 'well', '.', 'paxton', 'nice', 'job', 'show', 'knack', 'effective', 'directorial', 'flourish', '(', 'love', 'rain', '-', 'soak', 'montage', 'action', 'day', 'CARDINAL', 'open', ')', 'propel', 'plot', 'add', 'unexpected', 'psychological', 'depth', 'proceeding', '.', \"'s\", 'compelling', 'character', 'development', 'british', 'harry', 'vardon', 'haunt', 'image', 'aristocrats', 'black', 'suit', 'top', 'hat', 'destroy', 'family', 'cottage', 'child', 'make', 'way', 'golf', 'course', '.', 'also', 'good', 'job', 'visually', 'depict', 'go', 'player', \"'\", 'head', 'pressure', '.', 'golf', ',', 'painfully', 'bore', 'sport', ',', 'bring', 'vividly', 'alive', '.', 'credit', 'also', 'give', 'set', 'designer', 'costume', 'department', 'create', 'engage', 'period', '-', 'piece', 'atmosphere', 'london', 'boston', 'begin', 'DATE', 'DATE', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'know', 'go', 'end', \"'s\", 'base', 'true', 'story', 'also', 'film', 'genre', 'follow', 'template', ',', 'paxton', 'put', 'well', 'average', 'show', 'perhaps', 'indicate', 'talent', 'behind', 'camera', 'ever', 'front', '.', 'despite', 'formulaic', 'nature', ',', 'nice', 'easy', 'film', 'root', 'deserve', 'find', 'audience', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2425fdfa",
   "metadata": {},
   "source": [
    "#### performing stemming(ours-mystemDic) \n",
    "\n",
    "['go', 'see', 'movie', 'time', 'time', 'coax', 'friend', '-pron-', '.', 'will', 'admit', 'reluctant', 'see', 'know', 'ashton', 'kutcher', 'able', 'comedy', '.', 'wrong', '.', 'kutcher', 'play', 'character', 'jake', 'fischer', 'well', ',', 'kevin', 'costn', 'play', 'ben', 'randall', 'professionalism', '.', 'sign', 'good', 'movie', 'toy', 'emotion', '.', 'one', 'exactly', '.', 'entire', 'theater', '(', 'sell', ')', 'overcome', 'laught', 'ordinal', 'cardinal', 'movie', ',', 'move', 'tear', 'ordinal', 'cardinal', '.', 'exit', 'theater', 'see', 'many', 'woman', 'tear', ',', 'many', 'full', 'grown', 'man', 'well', ',', 'try', 'desperately', 'let', 'anyone', 'see', 'cry', '.', 'movie', 'great', ',', 'suggest', 'go', 'see', 'judge', '.']\n",
    "\n",
    "['actor', 'turn', 'director', 'bill', 'paxton', 'follow', 'promise', 'debut', ',', 'gothic', '-', 'horror', '`', '`', 'frailty', \"''\", ',', 'family', 'friendly', 'sport', 'drama', 'date', 'u.s', '.', 'open', 'young', 'norp', 'caddy', 'rise', 'humble', 'background', 'play', 'bristish', 'idol', 'dub', '`', '`', 'great', 'game', 'ever', 'play', '.', \"''\", \"'\", 'm', 'fan', 'golf', ',', 'scrappy', 'underdog', 'sport', 'flick', 'dime', 'dozen', '(', 'recently', 'do', 'grand', 'effect', '`', '`', 'miracle', \"''\", '`', '`', 'cinderella', 'man', \"''\", ')', ',', 'film', 'enthral', 'same', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'film', 'start', 'creative', 'opening', 'credit', '(', 'imagine', 'disneyfie', 'version', 'animate', 'open', 'credit', 'hbo', 'be', '`', '`', 'carnivale', \"''\", '`', '`', 'rome', \"''\", ')', ',', 'lumber', 'along', 'slowly', 'ordinal', 'by', '-', 'the', '-', 'number', 'hour', '.', 'action', 'move', 'u.s', '.', 'open', 'thing', 'pick', 'well', '.', 'paxton', 'nice', 'job', 'show', 'knack', 'effective', 'directorial', 'flourish', '(', 'love', 'rain', '-', 'soak', 'montage', 'action', 'day', 'cardinal', 'open', ')', 'propel', 'plot', 'add', 'unexpected', 'psychological', 'depth', 'proceeding', '.', 'be', 'compelling', 'character', 'development', 'british', 'harry', 'vardon', 'haunt', 'image', 'aristocrat', 'black', 'suit', 'top', 'hat', 'destroy', 'family', 'cottage', 'child', 'make', 'way', 'golf', 'course', '.', 'also', 'good', 'job', 'visually', 'depict', 'go', 'player', \"'\", 'head', 'pressure', '.', 'golf', ',', 'painfully', 'bore', 'sport', ',', 'br', 'vividly', 'alive', '.', 'credit', 'also', 'give', 'set', 'designer', 'costume', 'department', 'create', 'engage', 'period', '-', 'piece', 'atmosphere', 'london', 'boston', 'begin', 'date', 'date', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'know', 'go', 'end', 'be', 'base', 'true', 'story', 'also', 'film', 'genre', 'follow', 'template', ',', 'paxton', 'put', 'well', 'average', 'show', 'perhaps', 'indicate', 'talent', 'behind', 'camera', 'ever', 'front', '.', 'despite', 'formulaic', 'nature', ',', 'nice', 'easy', 'film', 'root', 'deserve', 'find', 'audience', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8fd58e",
   "metadata": {},
   "source": [
    "#### performing stemming(porter stemmer) \n",
    "\n",
    "['go', 'see', 'movi', 'time', 'time', 'coax', 'friend', '-pron-', '.', 'will', 'admit', 'reluct', 'see', 'know', 'ashton', 'kutcher', 'abl', 'comedi', '.', 'wrong', '.', 'kutcher', 'play', 'charact', 'jake', 'fischer', 'well', ',', 'kevin', 'costn', 'play', 'ben', 'randal', 'profession', '.', 'sign', 'good', 'movi', 'toy', 'emot', '.', 'one', 'exactli', '.', 'entir', 'theater', '(', 'sell', ')', 'overcom', 'laughter', 'ordin', 'cardin', 'movi', ',', 'move', 'tear', 'ordin', 'cardin', '.', 'exit', 'theater', 'see', 'mani', 'woman', 'tear', ',', 'mani', 'full', 'grown', 'man', 'well', ',', 'tri', 'desper', 'let', 'anyon', 'see', 'cri', '.', 'movi', 'great', ',', 'suggest', 'go', 'see', 'judg', '.']\n",
    "\n",
    "['actor', 'turn', 'director', 'bill', 'paxton', 'follow', 'promis', 'debut', ',', 'gothic', '-', 'horror', '`', '`', 'frailti', \"''\", ',', 'famili', 'friendli', 'sport', 'drama', 'date', 'u.', '.', 'open', 'young', 'norp', 'caddi', 'rise', 'humbl', 'background', 'play', 'bristish', 'idol', 'dub', '`', '`', 'great', 'game', 'ever', 'play', '.', \"''\", \"'\", 'm', 'fan', 'golf', ',', 'scrappi', 'underdog', 'sport', 'flick', 'dime', 'dozen', '(', 'recent', 'do', 'grand', 'effect', '`', '`', 'miracl', \"''\", '`', '`', 'cinderella', 'man', \"''\", ')', ',', 'film', 'enthral', 'same', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'film', 'start', 'creativ', 'open', 'credit', '(', 'imagin', 'disneyfi', 'version', 'anim', 'open', 'credit', 'hbo', \"'s\", '`', '`', 'carnival', \"''\", '`', '`', 'rome', \"''\", ')', ',', 'lumber', 'along', 'slowli', 'ordin', 'by', '-', 'the', '-', 'number', 'hour', '.', 'action', 'move', 'u.', '.', 'open', 'thing', 'pick', 'well', '.', 'paxton', 'nice', 'job', 'show', 'knack', 'effect', 'directori', 'flourish', '(', 'love', 'rain', '-', 'soak', 'montag', 'action', 'day', 'cardin', 'open', ')', 'propel', 'plot', 'add', 'unexpect', 'psycholog', 'depth', 'proceed', '.', \"'s\", 'compel', 'charact', 'develop', 'british', 'harri', 'vardon', 'haunt', 'imag', 'aristocrat', 'black', 'suit', 'top', 'hat', 'destroy', 'famili', 'cottag', 'child', 'make', 'way', 'golf', 'cours', '.', 'also', 'good', 'job', 'visual', 'depict', 'go', 'player', \"'\", 'head', 'pressur', '.', 'golf', ',', 'pain', 'bore', 'sport', ',', 'bring', 'vividli', 'aliv', '.', 'credit', 'also', 'give', 'set', 'design', 'costum', 'depart', 'creat', 'engag', 'period', '-', 'piec', 'atmospher', 'london', 'boston', 'begin', 'date', 'date', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'know', 'go', 'end', \"'s\", 'base', 'true', 'stori', 'also', 'film', 'genr', 'follow', 'templat', ',', 'paxton', 'put', 'well', 'averag', 'show', 'perhap', 'indic', 'talent', 'behind', 'camera', 'ever', 'front', '.', 'despit', 'formula', 'natur', ',', 'nice', 'easi', 'film', 'root', 'deserv', 'find', 'audienc', '.']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee5599",
   "metadata": {},
   "source": [
    "### obseravtions\n",
    "1. we can observe that our stemmer is performing well than porter stemmer.\n",
    "1. I have added Possessive ending 's, modal 'll etc.. in my dictionary which helped in good stemming\n",
    "1. we can observe that many words like movi(e),reluct(ant),abl(e),comedi(y),charact(er),emot(ion),exactli(y),overcome(e),orsin(al),cardin(al),mani(y),tri(y) etc...\n",
    "1. So I preferd using our stemmer in my project rather than porter stemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390ea33",
   "metadata": {},
   "source": [
    "#### After removing punctuations and br(html)\n",
    "\n",
    "['go', 'see', 'movie', 'time', 'time', 'coax', 'friend', 'pron', 'will', 'admit', 'reluctant', 'see', 'know', 'ashton', 'kutcher', 'able', 'comedy', 'wrong', 'kutcher', 'play', 'character', 'jake', 'fischer', 'well', 'kevin', 'costn', 'play', 'ben', 'randall', 'professionalism', 'sign', 'good', 'movie', 'toy', 'emotion', 'one', 'exactly', 'entire', 'theater', 'sell', 'overcome', 'laught', 'ordinal', 'cardinal', 'movie', 'move', 'tear', 'ordinal', 'cardinal', 'exit', 'theater', 'see', 'many', 'woman', 'tear', 'many', 'full', 'grown', 'man', 'well', 'try', 'desperately', 'let', 'anyone', 'see', 'cry', 'movie', 'great', 'suggest', 'go', 'see', 'judge']\n",
    "\n",
    "['actor', 'turn', 'director', 'bill', 'paxton', 'follow', 'promise', 'debut', 'gothic', 'horror', 'frailty', 'family', 'friendly', 'sport', 'drama', 'date', 'u.s', 'open', 'young', 'norp', 'caddy', 'rise', 'humble', 'background', 'play', 'bristish', 'idol', 'dub', 'great', 'game', 'ever', 'play', 'm', 'fan', 'golf', 'scrappy', 'underdog', 'sport', 'flick', 'dime', 'dozen', 'recently', 'do', 'grand', 'effect', 'miracle', 'cinderella', 'man', 'film', 'enthral', 'same', 'film', 'start', 'creative', 'opening', 'credit', 'imagine', 'disneyfie', 'version', 'animate', 'open', 'credit', 'hbo', 'be', 'carnivale', 'rome', 'lumber', 'along', 'slowly', 'ordinal', 'by', 'the', 'number', 'hour', 'action', 'move', 'u.s', 'open', 'thing', 'pick', 'well', 'paxton', 'nice', 'job', 'show', 'knack', 'effective', 'directorial', 'flourish', 'love', 'rain', 'soak', 'montage', 'action', 'day', 'cardinal', 'open', 'propel', 'plot', 'add', 'unexpected', 'psychological', 'depth', 'proceeding', 'be', 'compelling', 'character', 'development', 'british', 'harry', 'vardon', 'haunt', 'image', 'aristocrat', 'black', 'suit', 'top', 'hat', 'destroy', 'family', 'cottage', 'child', 'make', 'way', 'golf', 'course', 'also', 'good', 'job', 'visually', 'depict', 'go', 'player', 'head', 'pressure', 'golf', 'painfully', 'bore', 'sport', 'vividly', 'alive', 'credit', 'also', 'give', 'set', 'designer', 'costume', 'department', 'create', 'engage', 'period', 'piece', 'atmosphere', 'london', 'boston', 'begin', 'date', 'date', 'know', 'go', 'end', 'be', 'base', 'true', 'story', 'also', 'film', 'genre', 'follow', 'template', 'paxton', 'put', 'well', 'average', 'show', 'perhaps', 'indicate', 'talent', 'behind', 'camera', 'ever', 'front', 'despite', 'formulaic', 'nature', 'nice', 'easy', 'film', 'root', 'deserve', 'find', 'audience']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6dd5bf",
   "metadata": {},
   "source": [
    "## Using NLTK\n",
    "#### performing lemmatization and NER using NLTK (done before stopwords and infrequent words removal)\n",
    "\n",
    "['I', 'go', 'and', 'saw', 'this', 'movie', 'last', 'night', 'after', 'be', 'coax', 'to', 'by', 'a', 'few', 'friend', 'of', 'mine', '.', 'I', \"'ll\", 'admit', 'that', 'I', 'be', 'reluctant', 'to', 'see', 'it', 'because', 'from', 'what', 'I', 'know', 'of', 'PERSON', 'Kutcher', 'he', 'be', 'only', 'able', 'to', 'do', 'comedy', '.', 'I', 'be', 'wrong', '.', 'PERSON', 'play', 'the', 'character', 'of', 'PERSON', 'PERSON', 'very', 'well', ',', 'and', 'PERSON', 'PERSON', 'play', 'PERSON', 'PERSON', 'with', 'such', 'professionalism', '.', 'The', 'sign', 'of', 'a', 'good', 'movie', 'be', 'that', 'it', 'can', 'toy', 'with', 'our', 'emotion', '.', 'This', 'one', 'do', 'exactly', 'that', '.', 'The', 'entire', 'theater', '(', 'which', 'be', 'sell', 'out', ')', 'be', 'overcome', 'by', 'laughter', 'during', 'the', 'first', 'half', 'of', 'the', 'movie', ',', 'and', 'be', 'move', 'to', 'tear', 'during', 'the', 'second', 'half', '.', 'While', 'exit', 'the', 'theater', 'I', 'not', 'only', 'saw', 'many', 'woman', 'in', 'tear', ',', 'but', 'many', 'full', 'grow', 'men', 'a', 'well', ',', 'try', 'desperately', 'not', 'to', 'let', 'anyone', 'see', 'them', 'cry', '.', 'This', 'movie', 'be', 'great', ',', 'and', 'I', 'suggest', 'that', 'you', 'go', 'see', 'it', 'before', 'you', 'judge', '.']\n",
    "\n",
    "['PERSON', 'turn', 'director', 'PERSON', 'PERSON', 'follow', 'up', 'his', 'promising', 'debut', ',', 'the', 'Gothic-horror', '``', 'Frailty', \"''\", ',', 'with', 'this', 'family', 'friendly', 'sport', 'drama', 'about', 'the', '1913', 'GPE', 'Open', 'where', 'a', 'young', 'GPE', 'caddy', 'rise', 'from', 'his', 'humble', 'background', 'to', 'play', 'against', 'his', 'GPE', 'idol', 'in', 'what', 'be', 'dub', 'a', '``', 'The', 'GPE', 'Game', 'Ever', 'Played', '.', \"''\", 'I', \"'m\", 'no', 'fan', 'of', 'golf', ',', 'and', 'these', 'scrappy', 'underdog', 'sport', 'flick', 'be', 'a', 'dime', 'a', 'dozen', '(', 'most', 'recently', 'do', 'to', 'grand', 'effect', 'with', '``', 'Miracle', \"''\", 'and', '``', 'PERSON', 'PERSON', \"''\", ')', ',', 'but', 'some', 'how', 'this', 'film', 'be', 'enthral', 'all', 'the', 'same.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'The', 'film', 'start', 'with', 'some', 'creative', 'opening', 'credit', '(', 'imagine', 'a', 'Disneyfied', 'version', 'of', 'the', 'animated', 'opening', 'credit', 'of', 'ORGANIZATION', \"'s\", '``', 'Carnivale', \"''\", 'and', '``', 'Rome', \"''\", ')', ',', 'but', 'lumber', 'along', 'slowly', 'for', 'it', 'first', 'by-the-numbers', 'hour', '.', 'Once', 'the', 'action', 'move', 'to', 'the', 'GPE', 'Open', 'thing', 'pick', 'up', 'very', 'well', '.', 'PERSON', 'do', 'a', 'nice', 'job', 'and', 'show', 'a', 'knack', 'for', 'effective', 'directorial', 'flourish', '(', 'I', 'love', 'the', 'rain-soaked', 'montage', 'of', 'the', 'action', 'on', 'day', 'two', 'of', 'the', 'open', ')', 'that', 'propel', 'the', 'plot', 'far', 'or', 'add', 'some', 'unexpected', 'psychological', 'depth', 'to', 'the', 'proceeding', '.', 'There', \"'s\", 'some', 'compelling', 'character', 'development', 'when', 'the', 'GPE', 'PERSON', 'PERSON', 'be', 'haunt', 'by', 'image', 'of', 'the', 'aristocrat', 'in', 'black', 'suit', 'and', 'top', 'hat', 'who', 'destroy', 'his', 'family', 'cottage', 'a', 'a', 'child', 'to', 'make', 'way', 'for', 'a', 'golf', 'course', '.', 'He', 'also', 'do', 'a', 'good', 'job', 'of', 'visually', 'depict', 'what', 'go', 'on', 'in', 'the', 'player', \"'\", 'head', 'under', 'pressure', '.', 'PERSON', ',', 'a', 'painfully', 'boring', 'sport', ',', 'be', 'bring', 'vividly', 'alive', 'here', '.', 'PERSON', 'should', 'also', 'be', 'give', 'the', 'set', 'designer', 'and', 'costume', 'department', 'for', 'create', 'an', 'engage', 'period-piece', 'atmosphere', 'of', 'GPE', 'and', 'GPE', 'at', 'the', 'beginning', 'of', 'the', 'twentieth', 'century.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'You', 'know', 'how', 'this', 'be', 'go', 'to', 'end', 'not', 'only', 'because', 'it', \"'s\", 'base', 'on', 'a', 'true', 'story', 'but', 'also', 'because', 'film', 'in', 'this', 'genre', 'follow', 'the', 'same', 'template', 'over', 'and', 'over', ',', 'but', 'PERSON', 'put', 'on', 'a', 'good', 'than', 'average', 'show', 'and', 'perhaps', 'indicate', 'more', 'talent', 'behind', 'the', 'camera', 'than', 'he', 'ever', 'have', 'in', 'front', 'of', 'it', '.', 'Despite', 'the', 'formulaic', 'nature', ',', 'this', 'be', 'a', 'nice', 'and', 'easy', 'film', 'to', 'root', 'for', 'that', 'deserve', 'to', 'find', 'an', 'audience', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681d781",
   "metadata": {},
   "source": [
    "### Obseravtions:\n",
    "\n",
    "1. NLTK has only few NER's like GeoPoliticalEntities(GPE),PERSON,ORGANIZATION\n",
    "2. Space has many NER's like PERSON,NORP,FAC,ORG,GPE,LOC,PRODUCT,EVENT,WORK_OF_ART,LAW,LANGUAGE,DATE,TIME,PERCENT,\n",
    "MONEY,QUALTY,ORDINAL,CARDINAL.\n",
    "1. NLTK does NER(Named Entity Recognition) when first letter capitalized. For example:\n",
    " - Microsoft is considered as ORGANIZATION but microsoft is not\n",
    " - Devi is considered as PERSON but devi is not\n",
    "1. So I have to run NLTK NER before converting to lower case\n",
    "1. Lemmatization by NLTK doesnot perform well like Spacy. some words are not lemmatized properly.For ex:\n",
    " - saw not converted to see,men not converted to man etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23902d56",
   "metadata": {},
   "source": [
    "#### removed stopwords from lemmatized and ner done by NTLK\n",
    "\n",
    "['go', 'saw', 'movie', 'last', 'night', 'coax', 'friend', 'mine', '.', \"'ll\", 'admit', 'reluctant', 'see', 'know', 'person', 'kutcher', 'able', 'comedy', '.', 'wrong', '.', 'person', 'play', 'character', 'person', 'person', 'well', ',', 'person', 'person', 'play', 'person', 'person', 'professionalism', '.', 'sign', 'good', 'movie', 'toy', 'emotion', '.', 'one', 'exactly', '.', 'entire', 'theater', '(', 'sell', ')', 'overcome', 'laughter', 'first', 'half', 'movie', ',', 'move', 'tear', 'second', 'half', '.', 'exit', 'theater', 'saw', 'many', 'woman', 'tear', ',', 'many', 'full', 'grow', 'men', 'well', ',', 'try', 'desperately', 'let', 'anyone', 'see', 'cry', '.', 'movie', 'great', ',', 'suggest', 'go', 'see', 'judge', '.']\n",
    "\n",
    "['person', 'turn', 'director', 'person', 'person', 'follow', 'promising', 'debut', ',', 'gothic-horror', '``', 'frailty', '``', ',', 'family', 'friendly', 'sport', 'drama', '1913', 'gpe', 'open', 'young', 'gpe', 'caddy', 'rise', 'humble', 'background', 'play', 'gpe', 'idol', 'dub', '``', 'gpe', 'game', 'ever', 'played', '.', '``', \"'m\", 'fan', 'golf', ',', 'scrappy', 'underdog', 'sport', 'flick', 'dime', 'dozen', '(', 'recently', 'grand', 'effect', '``', 'miracle', '``', '``', 'person', 'person', '``', ')', ',', 'film', 'enthral', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'film', 'start', 'creative', 'opening', 'credit', '(', 'imagine', 'disneyfied', 'version', 'animated', 'opening', 'credit', 'organization', \"'s\", '``', 'carnivale', '``', '``', 'rome', '``', ')', ',', 'lumber', 'along', 'slowly', 'first', 'by-the-numbers', 'hour', '.', 'action', 'move', 'gpe', 'open', 'thing', 'pick', 'well', '.', 'person', 'nice', 'job', 'show', 'knack', 'effective', 'directorial', 'flourish', '(', 'love', 'rain-soaked', 'montage', 'action', 'day', 'two', 'open', ')', 'propel', 'plot', 'far', 'add', 'unexpected', 'psychological', 'depth', 'proceeding', '.', \"'s\", 'compelling', 'character', 'development', 'gpe', 'person', 'person', 'haunt', 'image', 'aristocrat', 'black', 'suit', 'top', 'hat', 'destroy', 'family', 'cottage', 'child', 'make', 'way', 'golf', 'course', '.', 'also', 'good', 'job', 'visually', 'depict', 'go', 'player', \"'\", 'head', 'pressure', '.', 'person', ',', 'painfully', 'boring', 'sport', ',', 'bring', 'vividly', 'alive', '.', 'person', 'also', 'give', 'set', 'designer', 'costume', 'department', 'create', 'engage', 'period-piece', 'atmosphere', 'gpe', 'gpe', 'beginning', 'twentieth', 'century', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'know', 'go', 'end', \"'s\", 'base', 'true', 'story', 'also', 'film', 'genre', 'follow', 'template', ',', 'person', 'put', 'good', 'average', 'show', 'perhaps', 'indicate', 'talent', 'behind', 'camera', 'ever', 'front', '.', 'despite', 'formulaic', 'nature', ',', 'nice', 'easy', 'film', 'root', 'deserve', 'find', 'audience', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1126f46",
   "metadata": {},
   "source": [
    "#### stemming using our stemmer\n",
    "\n",
    "['go', 'saw', 'movie', 'last', 'night', 'coax', 'friend', 'mine', '.', 'be', 'admit', 'reluctant', 'see', 'know', 'person', 'kutch', 'able', 'comedy', '.', 'wrong', '.', 'person', 'play', 'charact', 'person', 'person', 'well', ',', 'person', 'person', 'play', 'person', 'person', 'professionalism', '.', 'sign', 'good', 'movie', 'toy', 'emotion', '.', 'one', 'exactly', '.', 'entire', 'theater', '(', 'sell', ')', 'overcome', 'laughter', 'first', 'half', 'movie', ',', 'move', 'tear', 'second', 'half', '.', 'exit', 'theater', 'see', 'many', 'woman', 'tear', ',', 'many', 'full', 'grow', 'men', 'well', ',', 'try', 'desperately', 'let', 'anyone', 'see', 'cry', '.', 'movie', 'great', ',', 'suggest', 'go', 'see', 'judge', '.']\n",
    "\n",
    "['person', 'turn', 'director', 'person', 'person', 'follow', 'promising', 'debut', ',', 'gothic-horror', '``', 'frailty', '``', ',', 'family', 'friendly', 'sport', 'drama', '1913', 'gpe', 'open', 'young', 'gpe', 'caddy', 'rise', 'humble', 'background', 'play', 'gpe', 'idol', 'dub', '``', 'gpe', 'game', 'ever', 'play', '.', '``', 'am', 'fan', 'golf', ',', 'scrappy', 'underdog', 'sport', 'flick', 'dime', 'dozen', '(', 'recently', 'grand', 'effect', '``', 'miracle', '``', '``', 'person', 'person', '``', ')', ',', 'film', 'enthral', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'film', 'start', 'creative', 'opening', 'credit', '(', 'imagine', 'disneyfi', 'version', 'animat', 'open', 'credit', 'organization', 'be', '``', 'carnivale', '``', '``', 'rome', '``', ')', ',', 'lumber', 'along', 'slowly', 'first', 'by-the-number', 'hour', '.', 'action', 'move', 'gpe', 'open', 'thing', 'pick', 'well', '.', 'person', 'nice', 'job', 'show', 'knack', 'effective', 'directorial', 'flourish', '(', 'love', 'rain-soaked', 'montage', 'action', 'day', 'two', 'open', ')', 'propel', 'plot', 'far', 'add', 'unexpected', 'psychological', 'depth', 'proceeding', '.', 'be', 'compelling', 'character', 'development', 'gpe', 'person', 'person', 'haunt', 'image', 'aristocrat', 'black', 'suit', 'top', 'hat', 'destroy', 'family', 'cottage', 'child', 'make', 'way', 'golf', 'course', '.', 'also', 'good', 'job', 'visually', 'depict', 'go', 'player', \"'\", 'head', 'pressure', '.', 'person', ',', 'painfully', 'bor', 'sport', ',', 'br', 'vividly', 'alive', '.', 'person', 'also', 'give', 'set', 'designer', 'costume', 'department', 'create', 'engage', 'period-piece', 'atmosphere', 'gpe', 'gpe', 'beginn', 'twentieth', 'century', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'know', 'go', 'end', 'be', 'base', 'true', 'story', 'also', 'film', 'genre', 'follow', 'template', ',', 'person', 'put', 'good', 'average', 'show', 'perhaps', 'indicate', 'talent', 'behind', 'camera', 'ever', 'front', '.', 'despite', 'formulaic', 'nature', ',', 'nice', 'easy', 'film', 'root', 'deserve', 'find', 'audience', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb8cc0",
   "metadata": {},
   "source": [
    "#### stemming using porterstemmer \n",
    "\n",
    "['go', 'saw', 'movi', 'last', 'night', 'coax', 'friend', 'mine', '.', \"'ll\", 'admit', 'reluct', 'see', 'know', 'person', 'kutcher', 'abl', 'comedi', '.', 'wrong', '.', 'person', 'play', 'charact', 'person', 'person', 'well', ',', 'person', 'person', 'play', 'person', 'person', 'profession', '.', 'sign', 'good', 'movi', 'toy', 'emot', '.', 'one', 'exactli', '.', 'entir', 'theater', '(', 'sell', ')', 'overcom', 'laughter', 'first', 'half', 'movi', ',', 'move', 'tear', 'second', 'half', '.', 'exit', 'theater', 'saw', 'mani', 'woman', 'tear', ',', 'mani', 'full', 'grow', 'men', 'well', ',', 'tri', 'desper', 'let', 'anyon', 'see', 'cri', '.', 'movi', 'great', ',', 'suggest', 'go', 'see', 'judg', '.']\n",
    "\n",
    "['person', 'turn', 'director', 'person', 'person', 'follow', 'promis', 'debut', ',', 'gothic-horror', '``', 'frailti', '``', ',', 'famili', 'friendli', 'sport', 'drama', '1913', 'gpe', 'open', 'young', 'gpe', 'caddi', 'rise', 'humbl', 'background', 'play', 'gpe', 'idol', 'dub', '``', 'gpe', 'game', 'ever', 'play', '.', '``', \"'m\", 'fan', 'golf', ',', 'scrappi', 'underdog', 'sport', 'flick', 'dime', 'dozen', '(', 'recent', 'grand', 'effect', '``', 'miracl', '``', '``', 'person', 'person', '``', ')', ',', 'film', 'enthral', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'film', 'start', 'creativ', 'open', 'credit', '(', 'imagin', 'disneyfi', 'version', 'anim', 'open', 'credit', 'organ', \"'s\", '``', 'carnival', '``', '``', 'rome', '``', ')', ',', 'lumber', 'along', 'slowli', 'first', 'by-the-numb', 'hour', '.', 'action', 'move', 'gpe', 'open', 'thing', 'pick', 'well', '.', 'person', 'nice', 'job', 'show', 'knack', 'effect', 'directori', 'flourish', '(', 'love', 'rain-soak', 'montag', 'action', 'day', 'two', 'open', ')', 'propel', 'plot', 'far', 'add', 'unexpect', 'psycholog', 'depth', 'proceed', '.', \"'s\", 'compel', 'charact', 'develop', 'gpe', 'person', 'person', 'haunt', 'imag', 'aristocrat', 'black', 'suit', 'top', 'hat', 'destroy', 'famili', 'cottag', 'child', 'make', 'way', 'golf', 'cours', '.', 'also', 'good', 'job', 'visual', 'depict', 'go', 'player', \"'\", 'head', 'pressur', '.', 'person', ',', 'pain', 'bore', 'sport', ',', 'bring', 'vividli', 'aliv', '.', 'person', 'also', 'give', 'set', 'design', 'costum', 'depart', 'creat', 'engag', 'period-piec', 'atmospher', 'gpe', 'gpe', 'begin', 'twentieth', 'centuri', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'know', 'go', 'end', \"'s\", 'base', 'true', 'stori', 'also', 'film', 'genr', 'follow', 'templat', ',', 'person', 'put', 'good', 'averag', 'show', 'perhap', 'indic', 'talent', 'behind', 'camera', 'ever', 'front', '.', 'despit', 'formula', 'natur', ',', 'nice', 'easi', 'film', 'root', 'deserv', 'find', 'audienc', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e5d35",
   "metadata": {},
   "source": [
    "#### After removing punctuations (from lemmatized,NER by NLTK)\n",
    "['go', 'saw', 'movie', 'last', 'night', 'coax', 'friend', 'mine', 'be', 'admit', 'reluctant', 'see', 'know', 'person', 'kutch', 'able', 'comedy', 'wrong', 'person', 'play', 'charact', 'person', 'person', 'well', 'person', 'person', 'play', 'person', 'person', 'professionalism', 'sign', 'good', 'movie', 'toy', 'emotion', 'one', 'exactly', 'entire', 'theater', 'sell', 'overcome', 'laughter', 'first', 'half', 'movie', 'move', 'tear', 'second', 'half', 'exit', 'theater', 'see', 'many', 'woman', 'tear', 'many', 'full', 'grow', 'men', 'well', 'try', 'desperately', 'let', 'anyone', 'see', 'cry', 'movie', 'great', 'suggest', 'go', 'see', 'judge']\n",
    "\n",
    "['person', 'turn', 'director', 'person', 'person', 'follow', 'promising', 'debut', 'gothic', 'horror', 'frailty', 'family', 'friendly', 'sport', 'drama', '1913', 'gpe', 'open', 'young', 'gpe', 'caddy', 'rise', 'humble', 'background', 'play', 'gpe', 'idol', 'dub', 'gpe', 'game', 'ever', 'play', 'am', 'fan', 'golf', 'scrappy', 'underdog', 'sport', 'flick', 'dime', 'dozen', 'recently', 'grand', 'effect', 'miracle', 'person', 'person', 'film', 'enthral', 'film', 'start', 'creative', 'opening', 'credit', 'imagine', 'disneyfi', 'version', 'animat', 'open', 'credit', 'organization', 'be', 'carnivale', 'rome', 'lumber', 'along', 'slowly', 'first', 'by', 'the', 'number', 'hour', 'action', 'move', 'gpe', 'open', 'thing', 'pick', 'well', 'person', 'nice', 'job', 'show', 'knack', 'effective', 'directorial', 'flourish', 'love', 'rain', 'soaked', 'montage', 'action', 'day', 'two', 'open', 'propel', 'plot', 'far', 'add', 'unexpected', 'psychological', 'depth', 'proceeding', 'be', 'compelling', 'character', 'development', 'gpe', 'person', 'person', 'haunt', 'image', 'aristocrat', 'black', 'suit', 'top', 'hat', 'destroy', 'family', 'cottage', 'child', 'make', 'way', 'golf', 'course', 'also', 'good', 'job', 'visually', 'depict', 'go', 'player', 'head', 'pressure', 'person', 'painfully', 'bor', 'sport', 'vividly', 'alive', 'person', 'also', 'give', 'set', 'designer', 'costume', 'department', 'create', 'engage', 'period', 'piece', 'atmosphere', 'gpe', 'gpe', 'beginn', 'twentieth', 'century', 'know', 'go', 'end', 'be', 'base', 'true', 'story', 'also', 'film', 'genre', 'follow', 'template', 'person', 'put', 'good', 'average', 'show', 'perhaps', 'indicate', 'talent', 'behind', 'camera', 'ever', 'front', 'despite', 'formulaic', 'nature', 'nice', 'easy', 'film', 'root', 'deserve', 'find', 'audience']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
